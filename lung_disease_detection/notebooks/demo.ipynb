{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8f337c0-c343-414d-9d9d-a37902ba200f",
   "metadata": {},
   "source": [
    "# From Notebook to Kubeflow Pipeline using Lung Disease Detection\n",
    "\n",
    "In this notebook, we will walk you through the steps of converting a machine learning model, which you may already have on a jupyter notebook, into a Kubeflow pipeline. As an example, we will make use of thelung diesease detection use case.\n",
    "\n",
    "In this example we use:\n",
    "\n",
    "* **Kubeflow pipelines** - [Kubeflow Pipelines](https://www.kubeflow.org/docs/pipelines/overview/pipelines-overview/) is a machine learning workflow platform that is helping data scientists and ML engineers tackle experimentation and productionization of ML workloads. It allows users to easily orchestrate scalable workloads using an SDK right from the comfort of a Jupyter Notebook.\n",
    "\n",
    "**Note:** This notebook is to be run on a notebook server inside the Kubeflow environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f69470d-0fe0-42db-98ea-61ee4bcae4ab",
   "metadata": {},
   "source": [
    "## Section 1: ML Origin Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e27d20-02f2-47af-b07f-a9ec9e3191f2",
   "metadata": {},
   "source": [
    "Take Lung Disease Diagnosis as aan example which used to detect multiple type of disease from X-ray images.\n",
    "We chose Hernia model only with small datasets to demonstrate the funtionality of Kubeflow Pipelines without introducing too much complexity in the implementation of the ML model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5f6833-0e0b-475e-835f-0cd364a115ad",
   "metadata": {},
   "source": [
    "### 1.1 Install packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a22eb79-1c4e-4dff-87a8-5b4a9b687b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install --user --upgrade pip\n",
    "# !pip install --user --upgrade pandas matplotlib numpy tensorflow keras scikit-learn h5py Pillow kfp pyyaml dvc dvc[ssh]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0268037-a2af-441e-b028-a6157b2205d9",
   "metadata": {},
   "source": [
    "After the installation, we need to restart kernel for changes to take effect:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefbf0b9-32db-4b1a-b271-3f56f02c1149",
   "metadata": {},
   "source": [
    "### 1.2 Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de1f0483-9bd1-415f-ae75-778b68b6522c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle, resample\n",
    "from sklearn.metrics import  confusion_matrix, f1_score, roc_auc_score, roc_curve, auc, classification_report, accuracy_score, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.applications import VGG16, VGG19, ResNet101V2, ResNet50V2, InceptionV3, InceptionResNetV2, NASNetLarge, DenseNet121, DenseNet169, DenseNet201, Xception\n",
    "from tensorflow.keras.optimizers import Nadam, SGD, RMSprop, Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.metrics import binary_accuracy, categorical_crossentropy\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.backend import clear_session\n",
    "import sys\n",
    "# sys.path.append('../data/') \n",
    "import argparse\n",
    "import joblib\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d0ca26-3ecb-4444-a5c8-0e343e37126b",
   "metadata": {},
   "source": [
    "### 1.2 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d30816a4-ea2f-46af-93f7-5a13af2dd500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '../data/processed/Hernia' created successfully\n",
      "                                         image_index  labels\n",
      "0  ../data/raw/Hernia_sample10/positive/00009759_...  Hernia\n",
      "1  ../data/raw/Hernia_sample10/positive/00021902_...  Hernia\n",
      "2  ../data/raw/Hernia_sample10/positive/00014404_...  Hernia\n",
      "3  ../data/raw/Hernia_sample10/positive/00012003_...  Hernia\n",
      "4  ../data/raw/Hernia_sample10/positive/00000284_...  Hernia\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "raw_csv_path = \"../data/raw/Hernia_sample10/dataset.csv\"\n",
    "df = pd.read_csv(raw_csv_path, sep=\",\", encoding='utf-8')\n",
    "df[\"image_index\"]= df.image_index.apply(lambda x: x.replace('./datasets-registry', '../data/raw'))\n",
    "\n",
    "processed_data_dir =\"../data/processed/Hernia\" \n",
    "try:\n",
    "    os.makedirs(processed_data_dir, exist_ok=True)\n",
    "    print(\"Directory '%s' created successfully\" %processed_data_dir)\n",
    "except OSError as error:\n",
    "    print(\"Directory '%s' can not be created\")\n",
    "\n",
    "df.to_csv(os.path.join(processed_data_dir, \"dataset.csv\"), sep=\",\", index=False)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d155e9ce-3fd0-4e5b-9070-a4e75f898e1a",
   "metadata": {},
   "source": [
    "### 1.3 Train And Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2238580-ec40-42a2-b48c-bcd9f97f6f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "# Training data - used for training the model\n",
    "# Validation data - used for tuning the hyperparameters and evaluate the models\n",
    "# Test data - used to test the model after the model has gone through initial vetting by the validation set.\n",
    "train_val_df, test_df = train_test_split(\n",
    "    df,\n",
    "    test_size = 0.2,\n",
    "    random_state = 123,\n",
    "    stratify=df['labels']\n",
    ")\n",
    "train_df, val_df = train_test_split(\n",
    "    train_val_df,\n",
    "    test_size = 0.2,\n",
    "    random_state = 123,\n",
    "    stratify=train_val_df['labels']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c91df8d-15ec-4311-91a3-12668e5be77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 validated image filenames belonging to 2 classes.\n",
      "Found 2 validated image filenames belonging to 2 classes.\n",
      "Found 4 validated image filenames belonging to 2 classes.\n",
      "#####################################\n",
      "indices {'0': 0, 'Hernia': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/dataframe_iterator.py:282: UserWarning: Found 4 invalid image filename(s) in x_col=\"image_index\". These filename(s) will be ignored.\n",
      "  .format(n_invalid, x_col)\n",
      "/opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/dataframe_iterator.py:282: UserWarning: Found 2 invalid image filename(s) in x_col=\"image_index\". These filename(s) will be ignored.\n",
      "  .format(n_invalid, x_col)\n"
     ]
    }
   ],
   "source": [
    "labels = train_df['labels'].unique()\n",
    "labels = list(labels)\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255.,\n",
    "                                    rotation_range=20, \n",
    "                                    width_shift_range=0.2, \n",
    "                                    height_shift_range=0.2, \n",
    "                                    shear_range=0.3,\n",
    "                                    zoom_range=0.3,\n",
    "                                    horizontal_flip=True, \n",
    "                                    vertical_flip=False,\n",
    "                                    fill_mode=\"nearest\")\n",
    "\n",
    "val_test_datagen = ImageDataGenerator(rescale=1./255.)\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(dataframe=train_df,\n",
    "                                                    #directory=IMG_PATH,\n",
    "                                                    x_col='image_index',\n",
    "                                                    y_col='labels',\n",
    "                                                    target_size=(224, 224),\n",
    "                                                    batch_size=32,\n",
    "                                                    class_mode='binary',\n",
    "                                                    seed = 42,\n",
    "                                                    shuffle=True,\n",
    "#                                                     classes=['0', 'Hernia'],\n",
    "                                                    classes = labels,\n",
    "                                                    interpolation='nearest')\n",
    "\n",
    "val_generator = val_test_datagen.flow_from_dataframe(dataframe=val_df,\n",
    "                                                        #directory=IMG_PATH,\n",
    "                                                        x_col='image_index',\n",
    "                                                        y_col='labels',\n",
    "                                                        target_size=(224, 224),\n",
    "                                                        batch_size=32,\n",
    "                                                        class_mode='binary',\n",
    "                                                        seed = 42,\n",
    "                                                        classes = labels,\n",
    "#                                                         classes=['0', 'Hernia'],\n",
    "                                                        shuffle=True)\n",
    "                                                    \n",
    "\n",
    "test_generator = val_test_datagen.flow_from_dataframe(dataframe=test_df,\n",
    "                                                        #directory=IMG_PATH,\n",
    "                                                        x_col='image_index',\n",
    "                                                        y_col='labels',\n",
    "                                                        target_size=(224, 224),\n",
    "                                                        batch_size=1,\n",
    "                                                        class_mode='binary',\n",
    "                                                        classes = labels,\n",
    "#                                                         classes=['0', 'Hernia'],\n",
    "                                                        shuffle=False)\n",
    "print(\"#####################################\")\n",
    "print(\"indices\", train_generator.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc80eeb2-0e61-4e89-a305-54ad29b140d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '../models/Hernia' created successfully\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.9198 - accuracy: 0.6250\n",
      "Epoch 00001: val_loss improved from inf to 2.71308, saving model to ../models/Hernia/model.h5\n",
      "1/1 [==============================] - 23s 23s/step - loss: 0.9198 - accuracy: 0.6250 - val_loss: 2.7131 - val_accuracy: 0.5000\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4303 - accuracy: 0.7500\n",
      "Epoch 00002: val_loss did not improve from 2.71308\n",
      "1/1 [==============================] - 1s 698ms/step - loss: 0.4303 - accuracy: 0.7500 - val_loss: 5.9716 - val_accuracy: 0.5000\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7544 - accuracy: 0.7500\n",
      "Epoch 00003: val_loss did not improve from 2.71308\n",
      "1/1 [==============================] - 1s 691ms/step - loss: 0.7544 - accuracy: 0.7500 - val_loss: 9.7235 - val_accuracy: 0.5000\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3876 - accuracy: 0.8750\n",
      "Epoch 00004: val_loss did not improve from 2.71308\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "1/1 [==============================] - 1s 612ms/step - loss: 0.3876 - accuracy: 0.8750 - val_loss: 13.2379 - val_accuracy: 0.5000\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3289 - accuracy: 0.8750\n",
      "Epoch 00005: val_loss did not improve from 2.71308\n",
      "1/1 [==============================] - 1s 706ms/step - loss: 0.3289 - accuracy: 0.8750 - val_loss: 13.6113 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fdc24756290>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inceptresnet = InceptionResNetV2(\n",
    "    weights='imagenet',\n",
    "    input_shape=(224, 224, 3),\n",
    "    include_top=False)\n",
    "\n",
    "x = inceptresnet.output\n",
    "x = GlobalAveragePooling2D(name=\"gap\")(x)\n",
    "x = Dense(256, activation='elu', kernel_initializer='he_uniform')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "pred = Dense(1, activation = \"sigmoid\", name=\"fc_out\", kernel_initializer='he_uniform')(x)\n",
    "model = Model(inputs=inceptresnet.input, outputs=pred)\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_dir = \"../models/Hernia\" \n",
    "\n",
    "try:\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    print(\"Directory '%s' created successfully\" %model_dir)\n",
    "except OSError as error:\n",
    "    print(\"Directory '%s' can not be created\")\n",
    "\n",
    "model_saved_path = os.path.join(model_dir, \"model.h5\")\n",
    "\n",
    "checkpoint = ModelCheckpoint(model_saved_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1, mode='min')\n",
    "                            \n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min')\n",
    "\n",
    "callbacks_list = [checkpoint, reduce_lr, earlyStopping]\n",
    "\n",
    "model.fit(train_generator, \n",
    "        steps_per_epoch=math.ceil(train_generator.n/train_generator.batch_size),\n",
    "        epochs=5,\n",
    "        validation_data=val_generator,\n",
    "        validation_steps=math.ceil(val_generator.n/val_generator.batch_size),\n",
    "        callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d3ec8e6-152f-4df3-b73f-aa844c37520f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 1s 324ms/step\n"
     ]
    }
   ],
   "source": [
    "## predict\n",
    "if not model:\n",
    "    model = load_model(model_saved_path)\n",
    "    test_generator.reset()\n",
    "y_pred = model.predict(test_generator, steps=(math.ceil(test_generator.n/test_generator.batch_size)), verbose=1)\n",
    "y_true = test_generator.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8f19b31-d604-42ef-94e8-c316aad1b9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2 0 2\n",
      "AUC:  0.5\n",
      "F1-score:  0.667\n",
      "Sensitivity:  1.0\n",
      "Specificity:  0.0\n",
      "False positive rate: 1.0\n",
      "PPV:  0.5\n",
      "NPV:  nan\n",
      "Accuracy:  0.5\n",
      "Kappa Score:  0.0\n",
      "Directory '../reports/Hernia' created successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:23: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    }
   ],
   "source": [
    "#evaluation measures\n",
    "auc = roc_auc_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred >= 0.5)\n",
    "acc = accuracy_score(y_true, y_pred >= 0.5)\n",
    "#fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "#kappa is usually for imbalanced classes\n",
    "kappa_score = cohen_kappa_score(y_true, y_pred >= 0.5)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred >= 0.5)    \n",
    "#TN, FP, FN, TP = confusion_matrix(y_true, y_pred >= 0.5).ravel()\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "print (TN, FP, FN, TP)\n",
    "\n",
    "#sensitivity or true positive rate\n",
    "sensitivity = TP/(TP+FN)\n",
    "#specificity or true negative rate\n",
    "specificity = TN/(TN+FP)\n",
    "#false positive rate\n",
    "FPR = FP/(FP+TN)\n",
    "#precision, positive predictive value\n",
    "PPV = TP/(TP+FP)\n",
    "#negative predictive value\n",
    "NPV = TN/(TN+FN)\n",
    "\n",
    "# print(\"InceptionResNetV2 model (weights=%f, img_w=%f, img_h=%f, channel=%f):\" % (weights, img_w, img_h, channel))\n",
    "print ('AUC: ', round(auc, 3))\n",
    "print ('F1-score: ', round(f1, 3))\n",
    "print ('Sensitivity: ', round(sensitivity, 3))\n",
    "print ('Specificity: ', round(specificity, 3))\n",
    "print ('False positive rate:', round(FPR, 3))\n",
    "print ('PPV: ', round(PPV, 3))\n",
    "print ('NPV: ', round(NPV, 3))\n",
    "print ('Accuracy: ', round(acc, 3))\n",
    "print ('Kappa Score: ', round(kappa_score, 3))\n",
    "\n",
    "auc = auc.tolist()\n",
    "f1 = f1.tolist()\n",
    "acc = acc.tolist()\n",
    "cm = cm.tolist()\n",
    "\n",
    "report_dir = \"../reports/Hernia\" \n",
    "try:\n",
    "    os.makedirs(report_dir, exist_ok=True)\n",
    "    print(\"Directory '%s' created successfully\" %report_dir)\n",
    "except OSError as error:\n",
    "    print(\"Directory '%s' can not be created\")\n",
    "\n",
    "score_file = os.path.join(report_dir, \"scores.json\")\n",
    "with open(score_file, \"w\") as f:\n",
    "    scores = {\n",
    "        \"auc\": auc,\n",
    "        \"f1\": f1,\n",
    "        \"cm\": cm,\n",
    "        \"acc\": acc\n",
    "        }\n",
    "\n",
    "    json.dump(scores, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951d7c85-81be-474d-b372-8c72cd87d041",
   "metadata": {},
   "source": [
    "# Section 2: Kubeflow pipeline building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc4ff8f-53e9-4e30-9020-f21d5a0bc57d",
   "metadata": {},
   "source": [
    "Nex step, we will make use of the containerized approach provided by Kubeflow to allow our model to be run using Kubernetes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4aeca3-9e02-497a-a7a7-8c32a0fe2d9f",
   "metadata": {},
   "source": [
    "### 2.1 Install Kubeflow pipelines SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef92905-07d9-490e-9912-e4f0ebff4e08",
   "metadata": {},
   "source": [
    " The first step is to install the Kubeflow Pipelines SDK package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1dc2a130-746b-4105-987e-ecc7e8353fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --user --upgrade kfp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7056db2d-dc38-40a3-8e01-ddd18aed9967",
   "metadata": {},
   "source": [
    "After the installation, we need to restart kernel for changes to take effect:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334868f9-72cf-42c9-b96f-10e6cb4310cd",
   "metadata": {},
   "source": [
    "Check if the install was successful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ae47cd2-e1ad-4345-8661-d22c62ec9434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !which dsl-compile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7807486-9f01-464c-90e9-954352784701",
   "metadata": {},
   "source": [
    "You should see /usr/local/bin/dsl-compile above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1385acbf-bf3b-4f6a-bfb1-1adc27190f24",
   "metadata": {},
   "source": [
    "### 2.2 Build Container Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd41eca3-d87d-4809-b7ee-78387346f04a",
   "metadata": {},
   "source": [
    "The following cells define functions that will be transformed into lightweight container components. It is recommended to look at the corresponding Lung Disease Detection notebook to match what you see here to the original code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5008355e-208f-4e66-8a1f-50154bdb5222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94652060-4e61-497b-84f0-ed8de3e5d3b6",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr><td>\n",
    "    <img src=\"https://www.kubeflow.org/docs/images/pipelines-sdk-lightweight.svg\"\n",
    "         alt=\"Fashion MNIST sprite\"  width=\"600\">\n",
    "  </td></tr>\n",
    "  <tr><td align=\"center\">\n",
    "  </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd770221-869f-40d1-8ea5-988c8d752b5d",
   "metadata": {},
   "source": [
    "Import Kubeflow SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "309b23c3-b389-44b7-93de-abe4cf50575d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.components as comp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9507b18b-f369-498e-bded-25c89ed44942",
   "metadata": {},
   "source": [
    "Create standalone python function - load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70d0ce38-0dec-4ecb-b8c4-a14e2676219c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \n",
    "     # func_to_container_op requires packages to be imported inside of the function.\n",
    "    import os\n",
    "    import paramiko\n",
    "    import os\n",
    "    from os import walk\n",
    "    import pandas as pd\n",
    "    \n",
    "    ###### download datasets from ssh storage #######\n",
    "    # 1 - Open a transport\n",
    "    host=\"10.60.1.141\"\n",
    "    port = 22\n",
    "    transport = paramiko.Transport((host, port))\n",
    "\n",
    "    # 2 - Auth\n",
    "    password=\"P@ssw0rd\"\n",
    "    username=\"user\"\n",
    "    transport.connect(username = username, password = password)\n",
    "\n",
    "    # 3 - Go!\n",
    "    sftp = paramiko.SSHClient()\n",
    "    sftp._transport = transport\n",
    "    sftp_download = sftp.open_sftp()\n",
    "    # 4 - list all the files\n",
    "    source_folder=\"datasets-registry/Hernia_sample10\"\n",
    "    cmd_line = 'find '+source_folder+' ' +'-type f'\n",
    "    stdin, stdout, stderr = sftp.exec_command(cmd_line)\n",
    "    test = stdout.read().decode(\"utf-8\")\n",
    "    test1 = test.splitlines()\n",
    "    \n",
    "    # 5 - Download the files and put in the local folder\n",
    "    for file in test1:\n",
    "        print(file)\n",
    "        file_name = file.split('/')[-1]\n",
    "        if file_name:\n",
    "            base_path = file.split(file_name)[0]\n",
    "            if not os.path.exists(base_path):\n",
    "                os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "            if file_name.split('.')[-1] in ('csv'):\n",
    "                sftp_download.get(file, file)\n",
    "\n",
    "    df = pd.read_csv('datasets-registry/Hernia_sample10/dataset.csv', sep=\",\", encoding='utf-8')\n",
    "    print(df.head())\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05447f8-9d6a-4916-afcd-ff755564f496",
   "metadata": {},
   "source": [
    "Create standalone python function - train_and_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3383c24-9d9f-40d5-950b-14c25b713261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # func_to_container_op requires packages to be imported inside of the function.\n",
    "    import os\n",
    "    import math\n",
    "    import warnings\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.utils import shuffle, resample\n",
    "    from sklearn.metrics import  confusion_matrix, f1_score, roc_auc_score, roc_curve, auc, classification_report, accuracy_score, ConfusionMatrixDisplay\n",
    "    from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "    from tensorflow.keras.models import Sequential, load_model, Model\n",
    "    from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D, MaxPooling2D, BatchNormalization\n",
    "    from tensorflow.keras.applications import VGG16, VGG19, ResNet101V2, ResNet50V2, InceptionV3, InceptionResNetV2, NASNetLarge, DenseNet121, DenseNet169, DenseNet201, Xception\n",
    "    from tensorflow.keras.optimizers import Nadam, SGD, RMSprop, Adam\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "    from tensorflow.keras.metrics import binary_accuracy, categorical_crossentropy\n",
    "    from tensorflow.keras.regularizers import l2\n",
    "    from tensorflow.keras.backend import clear_session\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.utils import shuffle, resample\n",
    "    import sys\n",
    "    import argparse\n",
    "    import joblib\n",
    "    import json\n",
    "    \n",
    "    #defining the install function\n",
    "    import subprocess\n",
    "    def install(name):\n",
    "        subprocess.call(['pip', 'install', name])\n",
    "    \n",
    "    #install packages (installing numpy for the sake of demo)\n",
    "    install('paramiko')\n",
    "    import paramiko\n",
    "    \n",
    "    host=\"10.60.1.141\"\n",
    "    port = 22\n",
    "    transport = paramiko.Transport((host, port))\n",
    "\n",
    "    # 2 - Auth\n",
    "    password=\"P@ssw0rd\"\n",
    "    username=\"user\"\n",
    "    transport.connect(username = username, password = password)\n",
    "\n",
    "    # 3 - Go!\n",
    "    sftp = paramiko.SSHClient()\n",
    "    sftp._transport = transport\n",
    "    sftp_download = sftp.open_sftp()\n",
    "    # 4 - list all the files\n",
    "    source_folder=\"datasets-registry/Hernia_sample10\"\n",
    "    cmd_line = 'find '+source_folder+' ' +'-type f'\n",
    "    stdin, stdout, stderr = sftp.exec_command(cmd_line)\n",
    "    test = stdout.read().decode(\"utf-8\")\n",
    "    test1 = test.splitlines()\n",
    "    \n",
    "    # 5 - Download the files and put in the local folder\n",
    "    for file in test1:\n",
    "        print(file)\n",
    "        file_name = file.split('/')[-1]\n",
    "        if file_name:\n",
    "            base_path = file.split(file_name)[0]\n",
    "            if not os.path.exists(base_path):\n",
    "                os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "            if file_name.split('.')[-1] in ('jpg', 'png'):\n",
    "                sftp_download.get(file, file)\n",
    "    \n",
    "    df = pd.read_csv(f'{data_path}/dataset.csv', sep=\",\")\n",
    "    print(df['labels'].value_counts())\n",
    "    \n",
    "    train_val_df, test_df = train_test_split(\n",
    "        df,\n",
    "        test_size = 0.2,\n",
    "        random_state = 123,\n",
    "        stratify=df['labels']\n",
    "    )\n",
    "    train_df, val_df = train_test_split(\n",
    "        train_val_df,\n",
    "        test_size = 0.2,\n",
    "        random_state = 123,\n",
    "        stratify=train_val_df['labels']\n",
    "    )\n",
    "\n",
    "    # when use v1\n",
    "    # check whether need to do oversampling for positive datasets \n",
    "    pos_num = len(df[df[\"labels\"] != '0'])\n",
    "    neg_num = len(df[df[\"labels\"] == '0'])\n",
    "    print(\"positive:\", pos_num, \"negative:\", neg_num)\n",
    "\n",
    "    if pos_num+100 < neg_num:\n",
    "        # do oversampling\n",
    "        selected_df = pd.DataFrame(columns=['image_index', 'labels'])\n",
    "        pos_sub_df = train_df[train_df[\"labels\"] != '0']\n",
    "        neg_sub_df = train_df[train_df[\"labels\"] == '0']\n",
    "        # print(\"negative size\", len(neg_sub_df))\n",
    "        pos_tem_df = resample(pos_sub_df,\n",
    "                               replace=True,\n",
    "                               n_samples=2500,\n",
    "                               random_state=10)\n",
    "        selected_df = selected_df.append(pos_tem_df[['image_index', 'labels']])\n",
    "        \n",
    "        \n",
    "        neg_tem_df = resample(neg_sub_df,\n",
    "                                   replace=True,\n",
    "                                   n_samples=2500,\n",
    "                                   random_state=10)\n",
    "        selected_df = selected_df.append(neg_tem_df[['image_index', 'labels']])\n",
    "        train_df = selected_df \n",
    "    \n",
    "    labels = train_df['labels'].unique()\n",
    "    labels = list(labels)\n",
    "    \n",
    "    train_datagen = ImageDataGenerator(rescale=1./255.,\n",
    "                                    rotation_range=20, \n",
    "                                    width_shift_range=0.2, \n",
    "                                    height_shift_range=0.2, \n",
    "                                    shear_range=0.3,\n",
    "                                    zoom_range=0.3,\n",
    "                                    horizontal_flip=True, \n",
    "                                    vertical_flip=False,\n",
    "                                    fill_mode=\"nearest\")\n",
    "\n",
    "    val_test_datagen = ImageDataGenerator(rescale=1./255.)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_dataframe(dataframe=train_df,\n",
    "                                                        #directory=IMG_PATH,\n",
    "                                                        x_col='image_index',\n",
    "                                                        y_col='labels',\n",
    "                                                        target_size=(224, 224),\n",
    "                                                        batch_size=32,\n",
    "                                                        class_mode='binary',\n",
    "                                                        seed = 42,\n",
    "                                                        shuffle=True,\n",
    "                                                        # classes=['0', 'Hernia'],\n",
    "                                                        classes = labels,\n",
    "                                                        interpolation='nearest')\n",
    "\n",
    "    val_generator = val_test_datagen.flow_from_dataframe(dataframe=val_df,\n",
    "                                                         #directory=IMG_PATH,\n",
    "                                                         x_col='image_index',\n",
    "                                                         y_col='labels',\n",
    "                                                         target_size=(224, 224),\n",
    "                                                         batch_size=32,\n",
    "                                                         class_mode='binary',\n",
    "                                                         seed = 42,\n",
    "                                                         classes = labels,\n",
    "                                                        #  classes=['0', 'Hernia'],\n",
    "                                                         shuffle=True\n",
    "                                                         )\n",
    "\n",
    "    test_generator = val_test_datagen.flow_from_dataframe(dataframe=test_df,\n",
    "                                                          #directory=IMG_PATH,\n",
    "                                                          x_col='image_index',\n",
    "                                                          y_col='labels',\n",
    "                                                          target_size=(224, 224),\n",
    "                                                          batch_size=1,\n",
    "                                                          class_mode='binary',\n",
    "                                                          classes = labels,\n",
    "                                                        #   classes=['0', 'Hernia'],\n",
    "                                                          shuffle=False)\n",
    "    print(\"#####################################\")\n",
    "    print(\"indices\", train_generator.class_indices)\n",
    "\n",
    "    inceptresnet = InceptionResNetV2(\n",
    "        weights='imagenet',\n",
    "        input_shape=(224, 224, 3),\n",
    "        include_top=False)\n",
    "\n",
    "    x = inceptresnet.output\n",
    "    x = GlobalAveragePooling2D(name=\"gap\")(x)\n",
    "    x = Dense(256, activation='elu', kernel_initializer='he_uniform')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    pred = Dense(1, activation = \"sigmoid\", name=\"fc_out\", kernel_initializer='he_uniform')(x)\n",
    "    model = Model(inputs=inceptresnet.input, outputs=pred)\n",
    "\n",
    "    optimizer = Adam(lr=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Define model saved path \n",
    "    model_dir = \"../models/Hernia\" \n",
    "    try:\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        print(\"Directory '%s' created successfully\" %model_dir)\n",
    "    except OSError as error:\n",
    "        print(\"Directory '%s' can not be created\")\n",
    "\n",
    "    model_saved_path = os.path.join(model_dir, \"model.h5\")\n",
    "\n",
    "    checkpoint = ModelCheckpoint(model_saved_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1, mode='min')\n",
    "                                \n",
    "    earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min')\n",
    "\n",
    "    callbacks_list = [checkpoint, reduce_lr, earlyStopping]\n",
    "\n",
    "    model.fit(train_generator, \n",
    "            steps_per_epoch=math.ceil(train_generator.n/train_generator.batch_size),\n",
    "            epochs=5,\n",
    "            validation_data=val_generator,\n",
    "            validation_steps=math.ceil(val_generator.n/val_generator.batch_size),\n",
    "            callbacks=callbacks_list)\n",
    "    \n",
    "    ## predict\n",
    "    if not model:\n",
    "        model = load_model(model_saved_path)\n",
    "        test_generator.reset()\n",
    "    y_pred = model.predict(test_generator, steps=(math.ceil(test_generator.n/test_generator.batch_size)), verbose=1)\n",
    "    y_true = test_generator.classes\n",
    "\n",
    "    #evaluation measures\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred >= 0.5)\n",
    "    acc = accuracy_score(y_true, y_pred >= 0.5)\n",
    "    #fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "    #kappa is usually for imbalanced classes\n",
    "    kappa_score = cohen_kappa_score(y_true, y_pred >= 0.5)\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred >= 0.5)    \n",
    "    #TN, FP, FN, TP = confusion_matrix(y_true, y_pred >= 0.5).ravel()\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    print (TN, FP, FN, TP)\n",
    "    \n",
    "    #sensitivity or true positive rate\n",
    "    sensitivity = TP/(TP+FN)\n",
    "    #specificity or true negative rate\n",
    "    specificity = TN/(TN+FP)\n",
    "    #false positive rate\n",
    "    FPR = FP/(FP+TN)\n",
    "    #precision, positive predictive value\n",
    "    PPV = TP/(TP+FP)\n",
    "    #negative predictive value\n",
    "    NPV = TN/(TN+FN)\n",
    "\n",
    "    # print(\"InceptionResNetV2 model (weights=%f, img_w=%f, img_h=%f, channel=%f):\" % (weights, img_w, img_h, channel))\n",
    "    print ('AUC: ', round(auc, 3))\n",
    "    print ('F1-score: ', round(f1, 3))\n",
    "    print ('Sensitivity: ', round(sensitivity, 3))\n",
    "    print ('Specificity: ', round(specificity, 3))\n",
    "    print ('False positive rate:', round(FPR, 3))\n",
    "    print ('PPV: ', round(PPV, 3))\n",
    "    print ('NPV: ', round(NPV, 3))\n",
    "    print ('Accuracy: ', round(acc, 3))\n",
    "    print ('Kappa Score: ', round(kappa_score, 3))\n",
    "    \n",
    "    auc = auc.tolist()\n",
    "    f1 = f1.tolist()\n",
    "    acc = acc.tolist()\n",
    "    cm = cm.tolist()\n",
    "    \n",
    "    # export to scores.json\n",
    "    report_dir = \"../reports/Hernia\" \n",
    "    try:\n",
    "        os.makedirs(report_dir, exist_ok=True)\n",
    "        print(\"Directory '%s' created successfully\" %report_dir)\n",
    "    except OSError as error:\n",
    "        print(\"Directory '%s' can not be created\")\n",
    "\n",
    "    score_file = os.path.join(report_dir, \"scores.json\")\n",
    "    with open(score_file, \"w\") as f:\n",
    "        scores = {\n",
    "            \"auc\": auc,\n",
    "            \"f1\": f1,\n",
    "            \"cm\": cm,\n",
    "            \"acc\": acc\n",
    "            }\n",
    "\n",
    "        json.dump(scores, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668b12e2-fe3b-447d-98e6-abeb281cdba9",
   "metadata": {},
   "source": [
    "Create train and predict lightweight components, converting functions to container operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b00602fe-3cd1-43c7-971d-5c792225c3ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe15ea01-eab9-406a-9e95-d0ee1805bd34",
   "metadata": {},
   "source": [
    "### 2.3 Build Kubeflow Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e71d9c5-23ec-420d-a31b-585a93652505",
   "metadata": {},
   "source": [
    "Our next step will be to create the various components that will make up the pipeline. Define the pipeline using the *@dsl.pipeline* decorator.\n",
    "\n",
    "The pipeline function is defined and includes a number of paramters that will be fed into our various components throughout execution. Kubeflow Pipelines are created decalaratively. This means that the code is not run until the pipeline is compiled. \n",
    "\n",
    "A [Persistent Volume Claim](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) can be quickly created using the [VolumeOp](https://) method to save and persist data between the components. Note that while this is a great method to use locally, you could also use a cloud bucket for your persistent storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625f8b15-5bca-4a4d-9fea-47973d0834c5",
   "metadata": {},
   "source": [
    "Define the pipeline and define parameters to be fed into pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9655cef9-2bdb-48a8-8c37-8b34f9facf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ec10e4-043f-4fd5-bb89-59640272e819",
   "metadata": {},
   "source": [
    "### 2.4 Run pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7e615a-18de-4445-837f-7a523fd838e1",
   "metadata": {},
   "source": [
    "Finally we feed our pipeline definition into the compiler and run it as an experiment. This will give us 2 links at the bottom that we can follow to the [Kubeflow Pipelines UI](https://www.kubeflow.org/docs/pipelines/overview/pipelines-overview/) where you can check logs, artifacts, inputs/outputs, and visually see the progress of your pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f51a6e-3b01-4c12-8a13-a83887e757c5",
   "metadata": {},
   "source": [
    "Define some environment variables which are to be used as inputs at various points in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6598f007-1a59-40d4-bd63-a2f712d19345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b448ad11-1b9e-4fe2-8c14-814f8bcb3185",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40002434-830a-45a1-abb5-d0d5f6c7d3cc",
   "metadata": {},
   "source": [
    "Create a client to enable communication with the Pipelines API server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2821fb25-2a95-4fc3-98b9-c8e05d18c6ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b5a481b-b0e9-47a3-b734-8d4e9b1fb193",
   "metadata": {},
   "source": [
    "Compile and Run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d1ae0630-52ce-4b95-815f-b64b9785917c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/kfp/dsl/_container_op.py:1039: FutureWarning: Please create reusable components instead of constructing ContainerOp instances directly. Reusable components are shareable, portable and have compatibility and support guarantees. Please see the documentation: https://www.kubeflow.org/docs/pipelines/sdk/component-development/#writing-your-component-definition-file The components can be created manually (or, in case of python, using kfp.components.create_component_from_func or func_to_container_op) and then loaded using kfp.components.load_component_from_file, load_component_from_uri or load_component_from_text: https://kubeflow-pipelines.readthedocs.io/en/stable/source/kfp.components.html#kfp.components.load_component_from_file\n",
      "  category=FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/755a7ad4-2661-46a5-8ce6-ed52c7c768dd\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/5a06ba65-fc77-47d6-91fa-865be3024c41\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d66f5ed-a923-49a4-9fd9-1a90cf52f77f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24c1426-8cf1-4286-a0e6-1669fe742699",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd5852c-e26b-4bc1-bc4b-2ffc3ca7c6ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
