{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f95faa0e-94d7-4bf2-ba1e-04e447e90025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Kubeflow SDK\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.components as comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769146f6-a823-4666-add3-7e653cb80745",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    \n",
    "     # func_to_container_op requires packages to be imported inside of the function.\n",
    "    import os\n",
    "    import paramiko\n",
    "    import os\n",
    "    from os import walk\n",
    "    import pandas as pd\n",
    "    \n",
    "    ###### download datasets from git #######\n",
    "    dataset_url = \"https://github.com/ZoieD/Hernia_X-ray_images_sample/raw/main/Hernia_sample10.tgz\"\n",
    "    data_dir = tf.keras.utils.get_file(origin=dataset_url,\n",
    "                               fname='Hernia_sample',\n",
    "                               untar=True)\n",
    "    data_dir = pathlib.Path(data_dir)\n",
    "    # 1 - Open a transport\n",
    "    host=\"10.60.1.141\"\n",
    "    port = 22\n",
    "    transport = paramiko.Transport((host, port))\n",
    "\n",
    "    # 2 - Auth\n",
    "    password=\"P@ssw0rd\"\n",
    "    username=\"user\"\n",
    "    transport.connect(username = username, password = password)\n",
    "\n",
    "    # 3 - Go!\n",
    "    sftp = paramiko.SSHClient()\n",
    "    sftp._transport = transport\n",
    "    sftp_download = sftp.open_sftp()\n",
    "    # 4 - list all the files\n",
    "    source_folder=\"datasets-registry/Hernia_sample10\"\n",
    "    cmd_line = 'find '+source_folder+' ' +'-type f'\n",
    "    stdin, stdout, stderr = sftp.exec_command(cmd_line)\n",
    "    test = stdout.read().decode(\"utf-8\")\n",
    "    test1 = test.splitlines()\n",
    "    # 5 - Download the files and put in the local folder\n",
    "    for file in test1:\n",
    "        print(file)\n",
    "        file_name = file.split('/')[-1]\n",
    "        if file_name:\n",
    "            base_path = file.split(file_name)[0]\n",
    "            if not os.path.exists(base_path):\n",
    "                os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "            if file_name.split('.')[-1] in ('csv'):\n",
    "                sftp_download.get(file, file)\n",
    "\n",
    "    df = pd.read_csv('datasets-registry/Hernia_sample10/dataset.csv', sep=\",\", encoding='utf-8')        \n",
    "\n",
    "    df.to_csv(f'{data_path}/dataset.csv', sep=\",\", index=False)\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62adfdfd-1e9c-43ec-9509-32c4b132f7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_path, model_file):\n",
    "    # func_to_container_op requires packages to be imported inside of the function.\n",
    "    import os\n",
    "    import math\n",
    "    import warnings\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.utils import shuffle, resample\n",
    "    from sklearn.metrics import  confusion_matrix, f1_score, roc_auc_score, roc_curve, auc, classification_report, accuracy_score, ConfusionMatrixDisplay\n",
    "    from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "    from tensorflow.keras.models import Sequential, load_model, Model\n",
    "    from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D, MaxPooling2D, BatchNormalization\n",
    "    from tensorflow.keras.applications import VGG16, VGG19, ResNet101V2, ResNet50V2, InceptionV3, InceptionResNetV2, NASNetLarge, DenseNet121, DenseNet169, DenseNet201, Xception\n",
    "    from tensorflow.keras.optimizers import Nadam, SGD, RMSprop, Adam\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "    from tensorflow.keras.metrics import binary_accuracy, categorical_crossentropy\n",
    "    from tensorflow.keras.regularizers import l2\n",
    "    from tensorflow.keras.backend import clear_session\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.utils import shuffle, resample\n",
    "    import sys\n",
    "    import argparse\n",
    "    import joblib\n",
    "    import json\n",
    "    \n",
    "    #defining the install function\n",
    "    import subprocess\n",
    "    def install(name):\n",
    "        subprocess.call(['pip', 'install', name])\n",
    "    \n",
    "    #install packages (installing numpy for the sake of demo)\n",
    "    install('paramiko')\n",
    "    import paramiko\n",
    "    \n",
    "    host=\"10.60.1.141\"\n",
    "    port = 22\n",
    "    transport = paramiko.Transport((host, port))\n",
    "\n",
    "    # 2 - Auth\n",
    "    password=\"P@ssw0rd\"\n",
    "    username=\"user\"\n",
    "    transport.connect(username = username, password = password)\n",
    "\n",
    "    # 3 - Go!\n",
    "    sftp = paramiko.SSHClient()\n",
    "    sftp._transport = transport\n",
    "    sftp_download = sftp.open_sftp()\n",
    "    # 4 - list all the files\n",
    "    source_folder=\"datasets-registry/Hernia_sample10\"\n",
    "    cmd_line = 'find '+source_folder+' ' +'-type f'\n",
    "    stdin, stdout, stderr = sftp.exec_command(cmd_line)\n",
    "    test = stdout.read().decode(\"utf-8\")\n",
    "    test1 = test.splitlines()\n",
    "    # 5 - Download the files and put in the local folder\n",
    "    \n",
    "    for file in test1:\n",
    "        print(file)\n",
    "        file_name = file.split('/')[-1]\n",
    "        if file_name:\n",
    "            base_path = file.split(file_name)[0]\n",
    "            if not os.path.exists(base_path):\n",
    "                os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "            if file_name.split('.')[-1] in ('jpg', 'png'):\n",
    "                sftp_download.get(file, file)\n",
    "    \n",
    "    df = pd.read_csv(f'{data_path}/dataset.csv', sep=\",\")\n",
    "    print(df['labels'].value_counts())\n",
    "    \n",
    "    train_val_df, test_df = train_test_split(\n",
    "        df,\n",
    "        test_size = 0.2,\n",
    "        random_state = 123,\n",
    "        stratify=df['labels']\n",
    "    )\n",
    "    train_df, val_df = train_test_split(\n",
    "        train_val_df,\n",
    "        test_size = 0.2,\n",
    "        random_state = 123,\n",
    "        stratify=train_val_df['labels']\n",
    "    )\n",
    "\n",
    "    # when use v1\n",
    "    # check whether need to do oversampling for positive datasets \n",
    "    pos_num = len(df[df[\"labels\"] != '0'])\n",
    "    neg_num = len(df[df[\"labels\"] == '0'])\n",
    "    print(\"positive:\", pos_num, \"negative:\", neg_num)\n",
    "\n",
    "    if pos_num+100 < neg_num:\n",
    "        # do oversampling\n",
    "        selected_df = pd.DataFrame(columns=['image_index', 'labels'])\n",
    "        pos_sub_df = train_df[train_df[\"labels\"] != '0']\n",
    "        neg_sub_df = train_df[train_df[\"labels\"] == '0']\n",
    "        # print(\"negative size\", len(neg_sub_df))\n",
    "        pos_tem_df = resample(pos_sub_df,\n",
    "                               replace=True,\n",
    "                               n_samples=2500,\n",
    "                               random_state=10)\n",
    "        selected_df = selected_df.append(pos_tem_df[['image_index', 'labels']])\n",
    "        \n",
    "        \n",
    "        neg_tem_df = resample(neg_sub_df,\n",
    "                                   replace=True,\n",
    "                                   n_samples=2500,\n",
    "                                   random_state=10)\n",
    "        selected_df = selected_df.append(neg_tem_df[['image_index', 'labels']])\n",
    "        train_df = selected_df \n",
    "    \n",
    "    labels = train_df['labels'].unique()\n",
    "    labels = list(labels)\n",
    "    \n",
    "    train_datagen = ImageDataGenerator(rescale=1./255.,\n",
    "                                    rotation_range=20, \n",
    "                                    width_shift_range=0.2, \n",
    "                                    height_shift_range=0.2, \n",
    "                                    shear_range=0.3,\n",
    "                                    zoom_range=0.3,\n",
    "                                    horizontal_flip=True, \n",
    "                                    vertical_flip=False,\n",
    "                                    fill_mode=\"nearest\")\n",
    "\n",
    "    val_test_datagen = ImageDataGenerator(rescale=1./255.)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_dataframe(dataframe=train_df,\n",
    "                                                        #directory=IMG_PATH,\n",
    "                                                        x_col='image_index',\n",
    "                                                        y_col='labels',\n",
    "                                                        target_size=(224, 224),\n",
    "                                                        batch_size=32,\n",
    "                                                        class_mode='binary',\n",
    "                                                        seed = 42,\n",
    "                                                        shuffle=True,\n",
    "                                                        # classes=['0', 'Hernia'],\n",
    "                                                        classes = labels,\n",
    "                                                        interpolation='nearest')\n",
    "\n",
    "    val_generator = val_test_datagen.flow_from_dataframe(dataframe=val_df,\n",
    "                                                         #directory=IMG_PATH,\n",
    "                                                         x_col='image_index',\n",
    "                                                         y_col='labels',\n",
    "                                                         target_size=(224, 224),\n",
    "                                                         batch_size=32,\n",
    "                                                         class_mode='binary',\n",
    "                                                         seed = 42,\n",
    "                                                         classes = labels,\n",
    "                                                        #  classes=['0', 'Hernia'],\n",
    "                                                         shuffle=True\n",
    "                                                         )\n",
    "\n",
    "    test_generator = val_test_datagen.flow_from_dataframe(dataframe=test_df,\n",
    "                                                          #directory=IMG_PATH,\n",
    "                                                          x_col='image_index',\n",
    "                                                          y_col='labels',\n",
    "                                                          target_size=(224, 224),\n",
    "                                                          batch_size=1,\n",
    "                                                          class_mode='binary',\n",
    "                                                          classes = labels,\n",
    "                                                        #   classes=['0', 'Hernia'],\n",
    "                                                          shuffle=False)\n",
    "    print(\"#####################################\")\n",
    "    print(\"indices\", train_generator.class_indices)\n",
    "\n",
    "    inceptresnet = InceptionResNetV2(\n",
    "        weights='imagenet',\n",
    "        input_shape=(224, 224, 3),\n",
    "        include_top=False)\n",
    "\n",
    "    x = inceptresnet.output\n",
    "    x = GlobalAveragePooling2D(name=\"gap\")(x)\n",
    "    x = Dense(256, activation='elu', kernel_initializer='he_uniform')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    pred = Dense(1, activation = \"sigmoid\", name=\"fc_out\", kernel_initializer='he_uniform')(x)\n",
    "    model = Model(inputs=inceptresnet.input, outputs=pred)\n",
    "\n",
    "    optimizer = Adam(lr=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    # model_file = \"models/Hernia\" -- supposed to pass in\n",
    "#     try:\n",
    "#         os.makedirs(model_file, exist_ok=True)\n",
    "#         print(\"Directory '%s' created successfully\" %model_file)\n",
    "#     except OSError as error:\n",
    "#         print(\"Directory '%s' can not be created\")\n",
    "\n",
    "#     model_saved_path = os.path.join(model_file, \"model.h5\")\n",
    "\n",
    "    checkpoint = ModelCheckpoint(f'{data_path}/{model_file}', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1, mode='min')\n",
    "                                \n",
    "    earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min')\n",
    "\n",
    "    callbacks_list = [checkpoint, reduce_lr, earlyStopping]\n",
    "\n",
    "    model.fit(train_generator, \n",
    "            steps_per_epoch=math.ceil(train_generator.n/train_generator.batch_size),\n",
    "            epochs=5,\n",
    "            validation_data=val_generator,\n",
    "            validation_steps=math.ceil(val_generator.n/val_generator.batch_size),\n",
    "            callbacks=callbacks_list)\n",
    "    \n",
    "    ## predict\n",
    "    if not model:\n",
    "        model = load_model(f'{data_path}/{model_file}')\n",
    "        test_generator.reset()\n",
    "    y_pred = model.predict(test_generator, steps=(math.ceil(test_generator.n/test_generator.batch_size)), verbose=1)\n",
    "    y_true = test_generator.classes\n",
    "\n",
    "    #evaluation measures\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred >= 0.5)\n",
    "    acc = accuracy_score(y_true, y_pred >= 0.5)\n",
    "    #fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "    #kappa is usually for imbalanced classes\n",
    "    kappa_score = cohen_kappa_score(y_true, y_pred >= 0.5)\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred >= 0.5)    \n",
    "    #TN, FP, FN, TP = confusion_matrix(y_true, y_pred >= 0.5).ravel()\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    print (TN, FP, FN, TP)\n",
    "    \n",
    "    #sensitivity or true positive rate\n",
    "    sensitivity = TP/(TP+FN)\n",
    "    #specificity or true negative rate\n",
    "    specificity = TN/(TN+FP)\n",
    "    #false positive rate\n",
    "    FPR = FP/(FP+TN)\n",
    "    #precision, positive predictive value\n",
    "    PPV = TP/(TP+FP)\n",
    "    #negative predictive value\n",
    "    NPV = TN/(TN+FN)\n",
    "\n",
    "    # print(\"InceptionResNetV2 model (weights=%f, img_w=%f, img_h=%f, channel=%f):\" % (weights, img_w, img_h, channel))\n",
    "    print ('AUC: ', round(auc, 3))\n",
    "    print ('F1-score: ', round(f1, 3))\n",
    "    print ('Sensitivity: ', round(sensitivity, 3))\n",
    "    print ('Specificity: ', round(specificity, 3))\n",
    "    print ('False positive rate:', round(FPR, 3))\n",
    "    print ('PPV: ', round(PPV, 3))\n",
    "    print ('NPV: ', round(NPV, 3))\n",
    "    print ('Accuracy: ', round(acc, 3))\n",
    "    print ('Kappa Score: ', round(kappa_score, 3))\n",
    "    \n",
    "    auc = auc.tolist()\n",
    "    f1 = f1.tolist()\n",
    "    acc = acc.tolist()\n",
    "    cm = cm.tolist()\n",
    "\n",
    "    with open(f'{data_path}/scores.json', \"w\") as f:\n",
    "        scores = {\n",
    "            \"auc\": auc,\n",
    "            \"f1\": f1,\n",
    "            \"cm\": cm,\n",
    "            \"acc\": acc\n",
    "            }\n",
    "    \n",
    "        json.dump(scores, f, indent=4)\n",
    "    print(\"result saved successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34851b83-ea08-49e9-94cf-ac0c7e0187d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and predict lightweight components, converting functions to container operation\n",
    "load_data_op = comp.create_component_from_func(load_data, base_image='registry.hpe.com:5000/pipeline:load_data')\n",
    "train_op = comp.create_component_from_func(train, base_image='registry.hpe.com:5000/pipeline:train_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddecbee2-b3d7-4573-9d74-9eb2d561bd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "@dsl.pipeline(\n",
    "   name='Lung Disease Detection Pipeline',\n",
    "   description='Lung Disease Detection Pipeline to be executed on KubeFlow.'\n",
    ")\n",
    "\n",
    "# Define parameters to be fed into pipeline\n",
    "def lung_disease_detection_container_pipeline(\n",
    "    data_path: str, \n",
    "    model_file: str\n",
    "):\n",
    "    \n",
    "    # Define volume to share data between components.\n",
    "    vop = dsl.VolumeOp(\n",
    "    name=\"create_volume\",\n",
    "    resource_name=\"data-volume\", \n",
    "    size=\"1Gi\", \n",
    "    modes=dsl.VOLUME_MODE_RWM)\n",
    "\n",
    "    # Create lung disease detection:loading data component.\n",
    "    load_data_container = load_data_op(data_path) \\\n",
    "                                    .add_pvolumes({data_path: vop.volume})\n",
    "\n",
    "    # Create lung disease detection:train and evaluate component.\n",
    "    train_container = train_op(data_path, model_file) \\\n",
    "                                    .add_pvolumes({data_path: load_data_container.pvolume})\n",
    "\n",
    "    # Make a kfp.dsl.ContainerOp in Python that defines how Kubeflow Pipelines interacts with our container, specifying:\n",
    "            # The Docker image location to use\n",
    "            # How to pass arguments to the running container\n",
    "            # What outputs to expect from the container\n",
    "    #Print the metrics of the model. \n",
    "    lung_disease_detection_result_container = dsl.ContainerOp(\n",
    "        name=\"print_metrics\", # What will show up on the pipeline viewer\n",
    "        image='library/bash:4.4.23', # The image that KFP runs to do the work\n",
    "        pvolumes={data_path: train_container.pvolume},# Dictionary to match a path on the op’s fs with a V1Volume or it inherited type.\n",
    "        arguments=['cat', f'{data_path}/scores.json'] # the arguments of the command. \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dab3b17-c64c-42a5-847b-8eec3289df71",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/mnt'\n",
    "MODEL_FILE='hernia_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2661b081-c3e7-463c-8324-01f884abf6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_func = lung_disease_detection_container_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666776b5-7149-4335-b57b-144e65387449",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d49e643-d81f-4ec2-b0ab-c5bfc1436bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile and run the pipeline\n",
    "import time\n",
    "experiment_name = 'lung_disease_detection_pipeline'\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "namespace = 'zoie-workspace'\n",
    "\n",
    "arguments = {\"data_path\":DATA_PATH,\n",
    "             \"model_file\": MODEL_FILE}\n",
    "\n",
    "# Compile pipeline to generate compressed YAML definition of the pipeline.\n",
    "kfp.compiler.Compiler().compile(pipeline_func,  \n",
    "  '{}.zip'.format(experiment_name))\n",
    "\n",
    "# Submit pipeline directly from pipeline function \n",
    "run_result = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                                  experiment_name=experiment_name + '-' + time.strftime(\"%Y%m%d-%H%M%S\"), \n",
    "                                                  run_name=run_name, \n",
    "                                                  arguments=arguments,\n",
    "                                                  namespace=namespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df43590-562b-4c3f-9449-8e62131a171a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dfe7ae-1ad2-4248-8ba7-217e448ae292",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1376f1f4-eddd-4c03-9950-aabbed483114",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7a24cc-b3c1-495d-a86f-ad3d0505188b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35c7d24-e7dd-44f0-99bf-6b26ead6efbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
