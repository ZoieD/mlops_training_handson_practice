{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8f337c0-c343-414d-9d9d-a37902ba200f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# From Notebook to Kubeflow Pipeline using Lung Disease Detection\n",
    "\n",
    "In this notebook, we will walk you through the steps of converting a machine learning model, which you may already have on a jupyter notebook, into a Kubeflow pipeline. As an example, we will make use of Lung Disease Detection use case.\n",
    "\n",
    "In this example we use:\n",
    "\n",
    "* **Kubeflow pipelines** - [Kubeflow Pipelines](https://www.kubeflow.org/docs/pipelines/overview/pipelines-overview/) is a machine learning workflow platform that is helping data scientists and ML engineers tackle experimentation and productionization of ML workloads. It allows users to easily orchestrate scalable workloads using an SDK right from the comfort of a Jupyter Notebook.\n",
    "\n",
    "**Note:** This notebook is to be run on a notebook server inside the Kubeflow environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951d7c85-81be-474d-b372-8c72cd87d041",
   "metadata": {},
   "source": [
    "## Kubeflow pipeline building\n",
    "we will make use of the containerized approach provided by Kubeflow to allow our model to be run using Kubernetes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4aeca3-9e02-497a-a7a7-8c32a0fe2d9f",
   "metadata": {},
   "source": [
    "### 1. Install Kubeflow pipelines SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef92905-07d9-490e-9912-e4f0ebff4e08",
   "metadata": {},
   "source": [
    " The first step is to install the Kubeflow Pipelines SDK package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dc2a130-746b-4105-987e-ecc7e8353fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --user --upgrade kfp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7056db2d-dc38-40a3-8e01-ddd18aed9967",
   "metadata": {},
   "source": [
    "After the installation, we need to restart kernel for changes to take effect:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334868f9-72cf-42c9-b96f-10e6cb4310cd",
   "metadata": {},
   "source": [
    "Check if the install was successful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ae47cd2-e1ad-4345-8661-d22c62ec9434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !which dsl-compile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7807486-9f01-464c-90e9-954352784701",
   "metadata": {},
   "source": [
    "You should see /usr/local/bin/dsl-compile above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1385acbf-bf3b-4f6a-bfb1-1adc27190f24",
   "metadata": {},
   "source": [
    "### 2. Build Container Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd41eca3-d87d-4809-b7ee-78387346f04a",
   "metadata": {},
   "source": [
    "The following cells define functions that will be transformed into lightweight container components. It is recommended to look at the corresponding F notebook to match what you see here to the original code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5008355e-208f-4e66-8a1f-50154bdb5222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94652060-4e61-497b-84f0-ed8de3e5d3b6",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr><td>\n",
    "    <img src=\"https://www.kubeflow.org/docs/images/pipelines-sdk-lightweight.svg\"\n",
    "         alt=\"Fashion MNIST sprite\"  width=\"600\">\n",
    "  </td></tr>\n",
    "  <tr><td align=\"center\">\n",
    "  </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5983e53-11ed-463e-ad2c-a93d9e1e30b0",
   "metadata": {},
   "source": [
    "Components are self-contained pieces of code: Python functions.\n",
    "\n",
    "The function must be completely self-contained. No code (incl. imports) can be defined outside of the body itself. All imports must be included in the function body itself! Imported packages must be available in the base image.\n",
    "\n",
    "Why? Because each component will be packaged as a Docker image. The base image must therefore contain all dependencies. Any dependencies you install manually in the notebook are invisible to the Python function once it is inside the image. The function itself becomes the entrypoint of the image, which is why all auxiliary functions must be defined inside the function. That does cause some unfortunate duplication, but it also means you do not have to worry about the mechanism of packaging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306c5d06-b834-4ccd-87fd-5bce83aaa213",
   "metadata": {},
   "source": [
    "For this pipeline, we can define three components:\n",
    "\n",
    "- Download data set\n",
    "- Train model\n",
    "- Evaluate the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd770221-869f-40d1-8ea5-988c8d752b5d",
   "metadata": {},
   "source": [
    "##### Import Kubeflow SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "309b23c3-b389-44b7-93de-abe4cf50575d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "import kfp\n",
    "from kfp import dsl, components\n",
    "from kfp.components import InputBinaryFile, OutputBinaryFile, func_to_container_op, InputPath, OutputPath\n",
    "import time\n",
    "from functools import partial\n",
    "from kfserving import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb412ed-799f-44dc-ad4b-eb2ea2faec14",
   "metadata": {},
   "source": [
    "##### Define a fucntion to converts a Python function to a component and returns a task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58e7d82d-f5bb-4ab0-bf1c-f39b635aef60",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_to_container_op = partial(\n",
    "    components.func_to_container_op,\n",
    "    base_image='zdou001/only_tests:flower-nightly',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd2c1ad-7506-4c6f-9d0f-8d2e25a9cbe2",
   "metadata": {},
   "source": [
    "#### Component 1: Create standalone python function - load_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70d0ce38-0dec-4ecb-b8c4-a14e2676219c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@func_to_container_op\n",
    "def load_task(\n",
    "    dataset_url: str,\n",
    "    data_dir: OutputPath(str)\n",
    "):\n",
    "    \"\"\"Download Hernia X-ray image data\"\"\"\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    import urllib.request\n",
    "    import tarfile\n",
    "\n",
    "    Path(data_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    thetarfile = dataset_url\n",
    "    ftpstream = urllib.request.urlopen(thetarfile)\n",
    "    thetarfile = tarfile.open(fileobj=ftpstream, mode=\"r|gz\")\n",
    "    thetarfile.extractall(data_dir)\n",
    "    \n",
    "    print(f'data saved to {data_dir}/Hernia_sample50')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05447f8-9d6a-4916-afcd-ff755564f496",
   "metadata": {},
   "source": [
    "#### Component 2: Create standalone python function - train_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3383c24-9d9f-40d5-950b-14c25b713261",
   "metadata": {},
   "outputs": [],
   "source": [
    "@func_to_container_op\n",
    "def train_task(\n",
    "    data_dir: InputPath(str),\n",
    "    batch_size: int,\n",
    "    dropout_rate: float,\n",
    "    learning_rate: float,\n",
    "    epochs: int,\n",
    "    model_dir: OutputPath(str)):\n",
    "    \n",
    "    import math\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential, load_model, Model\n",
    "    from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D, MaxPooling2D, BatchNormalization\n",
    "    from tensorflow.keras.applications import VGG16, VGG19, ResNet101V2, ResNet50V2, InceptionV3, InceptionResNetV2, NASNetLarge, DenseNet121, DenseNet169, DenseNet201, Xception\n",
    "    from tensorflow.keras.optimizers import Nadam, SGD, RMSprop, Adam\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "\n",
    "    \"\"\"Split data\"\"\"\n",
    "    data_path = data_dir + '/Hernia_sample50' \n",
    "    df = pd.read_csv(f'{data_path}/dataset.csv', sep=\",\")\n",
    "    df[\"image_index\"]= df.image_index.apply(lambda x: x.replace('./datasets-registry', data_dir))\n",
    "    print(df)\n",
    "    print(df['labels'].value_counts())\n",
    "    \n",
    "    train_val_df, test_df = train_test_split(\n",
    "        df,\n",
    "        test_size = 0.2,\n",
    "        random_state = 123,\n",
    "        stratify=df['labels']\n",
    "    )\n",
    "    train_df, val_df = train_test_split(\n",
    "        train_val_df,\n",
    "        test_size = 0.2,\n",
    "        random_state = 123,\n",
    "        stratify=train_val_df['labels']\n",
    "    )\n",
    "    print(f\"Number of train set = {len(train_df)}\")\n",
    "    print(f\"Number of validation set = {len(val_df)}\")\n",
    "    print(f\"Number of test set = {len(test_df)}\")\n",
    "\n",
    "    \"\"\"Image Augmentation\"\"\"\n",
    "    labels = train_df['labels'].unique()\n",
    "    labels = list(labels)\n",
    "    \n",
    "    train_datagen = ImageDataGenerator(rescale=1./255.,\n",
    "                                        rotation_range=20, \n",
    "                                        width_shift_range=0.2, \n",
    "                                        height_shift_range=0.2, \n",
    "                                        shear_range=0.3,\n",
    "                                        zoom_range=0.3,\n",
    "                                        horizontal_flip=True, \n",
    "                                        vertical_flip=False,\n",
    "                                        fill_mode=\"nearest\")\n",
    "\n",
    "    val_test_datagen = ImageDataGenerator(rescale=1./255.)\n",
    "    \n",
    "    \"\"\"Create generators from data frame\"\"\"\n",
    "    train_generator = train_datagen.flow_from_dataframe(dataframe=train_df,\n",
    "                                                        x_col='image_index',\n",
    "                                                        y_col='labels',\n",
    "                                                        target_size=(224, 224),\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        class_mode='binary',\n",
    "                                                        seed = 42,\n",
    "                                                        shuffle=True,\n",
    "                                                        classes = labels,\n",
    "                                                        interpolation='nearest')\n",
    "\n",
    "    val_generator = val_test_datagen.flow_from_dataframe(dataframe=val_df,\n",
    "                                                            x_col='image_index',\n",
    "                                                            y_col='labels',\n",
    "                                                            target_size=(224, 224),\n",
    "                                                            batch_size=batch_size,\n",
    "                                                            class_mode='binary',\n",
    "                                                            seed = 42,\n",
    "                                                            classes = labels,\n",
    "                                                            shuffle=True)\n",
    "    \n",
    "    \"\"\"Define the model\"\"\"\n",
    "    inceptresnet = InceptionResNetV2(\n",
    "        weights='imagenet',\n",
    "        input_shape=(224, 224, 3),\n",
    "        include_top=False)\n",
    "\n",
    "    x = inceptresnet.output\n",
    "    x = GlobalAveragePooling2D(name=\"gap\")(x)\n",
    "    x = Dense(256, activation='elu', kernel_initializer='he_uniform')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    pred = Dense(1, activation = \"sigmoid\", name=\"fc_out\", kernel_initializer='he_uniform')(x)\n",
    "    model = Model(inputs=inceptresnet.input, outputs=pred)\n",
    "    \n",
    "    \"\"\"Train the model\"\"\"\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    Path(model_dir).mkdir(parents=True, exist_ok=True)\n",
    "    model_saved_path = os.path.join(model_dir, \"model.h5\")\n",
    "\n",
    "    checkpoint = ModelCheckpoint(model_saved_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1, mode='min')\n",
    "\n",
    "    earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min')\n",
    "\n",
    "    callbacks_list = [checkpoint, reduce_lr, earlyStopping]\n",
    "\n",
    "    model.fit(train_generator, \n",
    "            steps_per_epoch=math.ceil(train_generator.n/train_generator.batch_size),\n",
    "            epochs=epochs,\n",
    "            validation_data=val_generator,\n",
    "            validation_steps=math.ceil(val_generator.n/val_generator.batch_size),\n",
    "            callbacks=callbacks_list)\n",
    "    \n",
    "    model.summary()\n",
    "    print(f'Model exported to: {model_dir}')\n",
    "    print(os.listdir(model_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923623e3-a145-42a3-9794-89bdbd08e121",
   "metadata": {},
   "source": [
    "#### Component 3: Create standalone python function - evaluate_task()\n",
    "Evaluate the model with the following Python function. The metrics metadata (loss and accuracy) is available to the Kubeflow Pipelines UI. All metadata can automatically be visualized with output viewer(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b00602fe-3cd1-43c7-971d-5c792225c3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "@func_to_container_op\n",
    "def evaluate_task(\n",
    "    data_dir: InputPath(str),\n",
    "    model_dir: InputPath(str),\n",
    "    mlpipeline_metrics_path: OutputPath('Metrics')):\n",
    "    \"\"\"Loads a saved model from file and uses a pre-downloaded dataset for evaluation.\n",
    "    Model metrics are persisted to `/mlpipeline-metrics.json` for Kubeflow Pipelines\n",
    "    metadata.\"\"\"\n",
    "    \n",
    "    import math\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "    from tensorflow.keras.models import Sequential, load_model, Model\n",
    "    from sklearn.metrics import  confusion_matrix, f1_score, roc_auc_score, roc_curve, auc, classification_report, accuracy_score, ConfusionMatrixDisplay\n",
    "    from sklearn.metrics import cohen_kappa_score\n",
    "    import json\n",
    "    from collections import namedtuple\n",
    "\n",
    "    \"\"\"Load test Hernia dataset \"\"\"\n",
    "    data_path = data_dir + '/Hernia_sample50' \n",
    "    df = pd.read_csv(f'{data_path}/dataset.csv', sep=\",\")\n",
    "    df[\"image_index\"]= df.image_index.apply(lambda x: x.replace('./datasets-registry', data_dir))\n",
    "    print(df)\n",
    "    print(df['labels'].value_counts())\n",
    "    \n",
    "    train_val_df, test_df = train_test_split(\n",
    "        df,\n",
    "        test_size = 0.2,\n",
    "        random_state = 123,\n",
    "        stratify=df['labels']\n",
    "    )\n",
    "    train_df, val_df = train_test_split(\n",
    "        train_val_df,\n",
    "        test_size = 0.2,\n",
    "        random_state = 123,\n",
    "        stratify=train_val_df['labels']\n",
    "    )\n",
    "    \n",
    "    \"\"\"Image Augmentation\"\"\"\n",
    "    labels = train_df['labels'].unique()\n",
    "    labels = list(labels)\n",
    "    \n",
    "    val_test_datagen = ImageDataGenerator(rescale=1./255.)\n",
    "    \n",
    "    \"\"\"Create generators from data frame\"\"\"\n",
    "    test_generator = val_test_datagen.flow_from_dataframe(dataframe=test_df,\n",
    "                                                        x_col='image_index',\n",
    "                                                        y_col='labels',\n",
    "                                                        target_size=(224, 224),\n",
    "                                                        batch_size=1,\n",
    "                                                        class_mode='binary',\n",
    "                                                        classes = labels,\n",
    "                                                        shuffle=False)\n",
    "\n",
    "    \"\"\"Load model and evalutate using metrics\"\"\"\n",
    "    model_saved_path = os.path.join(model_dir, \"model.h5\")\n",
    "    reloaded = load_model(model_saved_path)\n",
    "    test_generator.reset()\n",
    "    y_pred = reloaded.predict(test_generator, steps=(math.ceil(test_generator.n/test_generator.batch_size)), verbose=1)\n",
    "    y_true = test_generator.classes\n",
    "    \n",
    "    reloaded.summary()\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred >= 0.5)\n",
    "    acc = accuracy_score(y_true, y_pred >= 0.5)\n",
    "    #fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "    #kappa is usually for imbalanced classes\n",
    "    kappa_score = cohen_kappa_score(y_true, y_pred >= 0.5)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred >= 0.5)    \n",
    "    #TN, FP, FN, TP = confusion_matrix(y_true, y_pred >= 0.5).ravel()\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    print (TN, FP, FN, TP)\n",
    "\n",
    "    #sensitivity or true positive rate\n",
    "    sensitivity = TP/(TP+FN)\n",
    "    #specificity or true negative rate\n",
    "    specificity = TN/(TN+FP)\n",
    "    #false positive rate\n",
    "    FPR = FP/(FP+TN)\n",
    "    #precision, positive predictive value\n",
    "    PPV = TP/(TP+FP)\n",
    "    #negative predictive value\n",
    "    NPV = TN/(TN+FN)\n",
    "\n",
    "    # print(\"InceptionResNetV2 model (weights=%f, img_w=%f, img_h=%f, channel=%f):\" % (weights, img_w, img_h, channel))\n",
    "    print ('AUC: ', round(auc, 3))\n",
    "    print ('F1-score: ', round(f1, 3))\n",
    "    print ('Sensitivity: ', round(sensitivity, 3))\n",
    "    print ('Specificity: ', round(specificity, 3))\n",
    "    print ('False positive rate:', round(FPR, 3))\n",
    "    print ('PPV: ', round(PPV, 3))\n",
    "    print ('NPV: ', round(NPV, 3))\n",
    "    print ('Accuracy: ', round(acc, 3))\n",
    "    print ('Kappa Score: ', round(kappa_score, 3))\n",
    "    \n",
    "        \n",
    "    # save results to json\n",
    "    metrics = []\n",
    "    metrics.append({\n",
    "            'f1': round(f1, 4),\n",
    "            'accuracy': round(acc, 4)\n",
    "        })\n",
    "    \n",
    "    metrics = []\n",
    "    f1_dict = {\n",
    "        'name': 'f1',\n",
    "        'numberValue': round(f1, 4),\n",
    "        'format': \"PERCENTAGE\"\n",
    "    }\n",
    "    acc_dict = {\n",
    "        'name': 'accuracy',\n",
    "        'numberValue': round(acc, 4),\n",
    "        'format': \"PERCENTAGE\"\n",
    "    }\n",
    "    metrics.append(f1_dict)\n",
    "    metrics.append(acc_dict)\n",
    "        \n",
    "    metrics_dict = {'metrics': metrics}\n",
    "    print(metrics_dict)\n",
    "\n",
    "    with open(mlpipeline_metrics_path, 'w') as f:\n",
    "        json.dump(metrics_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56803b7c-cfb9-4037-b02a-4adf2c5824d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. Combine the Components into a Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e694308a-2233-4af1-aefe-0622c0fa0fbc",
   "metadata": {},
   "source": [
    "Note that up to this point you have not yet used the Kubeflow Pipelines SDK!\n",
    "\n",
    "With the four components (i.e. self-contained functions) defined, wire up the dependencies with Kubeflow Pipelines.\n",
    "\n",
    "The call components.func_to_container_op(f, base_image=img)(*args) has the following ingredients:\n",
    "\n",
    "- `f` is the Python function that defines a component\n",
    "- `img` is the base (Docker) image used to package the function\n",
    "- `*arg`s lists the arguments to f\n",
    "\n",
    "What the `*args` mean is best explained by going forward through the graph:\n",
    "\n",
    "- `downloadOp` is the first step and has no dependencies; it therefore has no `InputPath`. Its output (i.e., `OutputPath`) is stored in `data_dir`\n",
    "- `trainOp` needs the data downloaded from `downloadOp` and its signature lists `data_dir` (input) and `model_dir` (output). It depends on `downloadOp.output` (i.e., the previous step’s output) and stores its own outputs in `model_dir`, which can be used by another step. `downloadOp` is the parent of `trainOp`, as required.\n",
    "- `evaluateOp`'s function takes three arguments: `data_dir` (i.e., `downloadOp.output`), `model_dir` (i.e., `trainOp.output`), and `metrics_path`, which is where the function stores its evaluation metrics. That way, `evaluateOp` can only run after the successful completion of both `downloadOp` and `trainOp`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe15ea01-eab9-406a-9e95-d0ee1805bd34",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3.1 Build Kubeflow Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e71d9c5-23ec-420d-a31b-585a93652505",
   "metadata": {},
   "source": [
    "Our next step will be to create the various components that will make up the pipeline. Define the pipeline using the *@dsl.pipeline* decorator.\n",
    "\n",
    "The pipeline function is defined and includes a number of paramters that will be fed into our various components throughout execution. Kubeflow Pipelines are created decalaratively. This means that the code is not run until the pipeline is compiled. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625f8b15-5bca-4a4d-9fea-47973d0834c5",
   "metadata": {},
   "source": [
    "Define the pipeline and define parameters to be fed into pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9655cef9-2bdb-48a8-8c37-8b34f9facf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name='Lung Disease Detection Pipeline',\n",
    "    description='Lung Disease Detection Pipeline to be executed on KubeFlow.',\n",
    ")\n",
    "def lung_disease_detection_pipeline(\n",
    "    dataset_url='https://github.com/ZoieD/Hernia_X-ray_images_sample/raw/main/Hernia_sample50.tgz',\n",
    "    batch_size=32,\n",
    "    dropout_rate=0.5,\n",
    "    learning_rate=0.001,\n",
    "    epochs=5,\n",
    "    namespace=utils.get_default_target_namespace(),       \n",
    "):\n",
    "    downloadOp = load_task(dataset_url)\n",
    "\n",
    "    trainOp = train_task(downloadOp.output, batch_size, dropout_rate, learning_rate, epochs)\n",
    "    trainOp.after(downloadOp)\n",
    "    # trainOp.container.set_gpu_limit(1)\n",
    "\n",
    "    evaluateOp = evaluate_task(downloadOp.output, trainOp.output)\n",
    "    # evaluateOp.after(trainOp)\n",
    "    # evaluateOp.container.set_gpu_limit(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ec10e4-043f-4fd5-bb89-59640272e819",
   "metadata": {},
   "source": [
    "#### 3.2 Run pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7e615a-18de-4445-837f-7a523fd838e1",
   "metadata": {},
   "source": [
    "Finally we feed our pipeline definition into the compiler and run it as an experiment. This will give us 2 links at the bottom that we can follow to the [Kubeflow Pipelines UI](https://www.kubeflow.org/docs/pipelines/overview/pipelines-overview/) where you can check logs, artifacts, inputs/outputs, and visually see the progress of your pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40002434-830a-45a1-abb5-d0d5f6c7d3cc",
   "metadata": {},
   "source": [
    "Create a client to enable communication with the Pipelines API server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2821fb25-2a95-4fc3-98b9-c8e05d18c6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[E 220920 13:41:35 _satvolumecredentials:51] Failed to read a token from file '/var/run/secrets/kubeflow/pipelines/token' ([Errno 2] No such file or directory: '/var/run/secrets/kubeflow/pipelines/token').\n",
      "[W 220920 13:41:35 _client:372] Failed to set up default credentials. Proceeding without credentials...\n"
     ]
    }
   ],
   "source": [
    "client = kfp.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5a481b-b0e9-47a3-b734-8d4e9b1fb193",
   "metadata": {},
   "source": [
    "Compile and Run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1ae0630-52ce-4b95-815f-b64b9785917c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/kfp/components/_data_passing.py:227: UserWarning: Missing type name was inferred as \"Integer\" based on the value \"32\".\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/kfp/components/_data_passing.py:227: UserWarning: Missing type name was inferred as \"Float\" based on the value \"0.5\".\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/kfp/components/_data_passing.py:227: UserWarning: Missing type name was inferred as \"Float\" based on the value \"0.001\".\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/kfp/components/_data_passing.py:227: UserWarning: Missing type name was inferred as \"Integer\" based on the value \"5\".\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/99a4f982-e098-4cec-9c5e-6b538f6f770d\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/e7d5d9cd-9d65-4055-acf6-3fb0de08b377\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kfp.compiler.Compiler().compile(lung_disease_detection_pipeline, 'lung_disease_detection_pipeline.yaml')\n",
    "pipeline_func=lung_disease_detection_pipeline\n",
    "namespace = utils.get_default_target_namespace()\n",
    "experiment_name = 'lung_disease_detection_pipeline'+'_'+namespace\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "run_result = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                              experiment_name=experiment_name, \n",
    "                                              run_name=run_name + '-' + time.strftime(\"%Y%m%d-%H%M%S\"), \n",
    "                                              arguments={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aab601-e51f-466d-8ab3-96031f26878d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24dc0b9-048c-489f-a80c-995ece2ad1fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
