{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8f337c0-c343-414d-9d9d-a37902ba200f",
   "metadata": {},
   "source": [
    "# From Notebook to Kubeflow Pipeline using Flower Classification\n",
    "\n",
    "In this notebook, we will walk you through the steps of converting a machine learning model, which you may already have on a jupyter notebook, into a Kubeflow pipeline. As an example, we will make use of flower classification use case.\n",
    "\n",
    "In this example we use:\n",
    "\n",
    "* **Kubeflow pipelines** - [Kubeflow Pipelines](https://www.kubeflow.org/docs/pipelines/overview/pipelines-overview/) is a machine learning workflow platform that is helping data scientists and ML engineers tackle experimentation and productionization of ML workloads. It allows users to easily orchestrate scalable workloads using an SDK right from the comfort of a Jupyter Notebook.\n",
    "\n",
    "**Note:** This notebook is to be run on a notebook server inside the Kubeflow environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951d7c85-81be-474d-b372-8c72cd87d041",
   "metadata": {},
   "source": [
    "## Kubeflow pipeline building\n",
    "we will make use of the containerized approach provided by Kubeflow to allow our model to be run using Kubernetes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4aeca3-9e02-497a-a7a7-8c32a0fe2d9f",
   "metadata": {},
   "source": [
    "### 1. Install Kubeflow pipelines SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef92905-07d9-490e-9912-e4f0ebff4e08",
   "metadata": {},
   "source": [
    " The first step is to install the Kubeflow Pipelines SDK package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dc2a130-746b-4105-987e-ecc7e8353fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --user --upgrade kfp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7056db2d-dc38-40a3-8e01-ddd18aed9967",
   "metadata": {},
   "source": [
    "After the installation, we need to restart kernel for changes to take effect:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334868f9-72cf-42c9-b96f-10e6cb4310cd",
   "metadata": {},
   "source": [
    "Check if the install was successful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ae47cd2-e1ad-4345-8661-d22c62ec9434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !which dsl-compile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7807486-9f01-464c-90e9-954352784701",
   "metadata": {},
   "source": [
    "You should see /usr/local/bin/dsl-compile above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1385acbf-bf3b-4f6a-bfb1-1adc27190f24",
   "metadata": {},
   "source": [
    "### 2. Build Container Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd41eca3-d87d-4809-b7ee-78387346f04a",
   "metadata": {},
   "source": [
    "The following cells define functions that will be transformed into lightweight container components. It is recommended to look at the corresponding Flower Classification notebook to match what you see here to the original code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5008355e-208f-4e66-8a1f-50154bdb5222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94652060-4e61-497b-84f0-ed8de3e5d3b6",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr><td>\n",
    "    <img src=\"https://www.kubeflow.org/docs/images/pipelines-sdk-lightweight.svg\"\n",
    "         alt=\"Fashion MNIST sprite\"  width=\"600\">\n",
    "  </td></tr>\n",
    "  <tr><td align=\"center\">\n",
    "  </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5983e53-11ed-463e-ad2c-a93d9e1e30b0",
   "metadata": {},
   "source": [
    "Components are self-contained pieces of code: Python functions.\n",
    "\n",
    "The function must be completely self-contained. No code (incl. imports) can be defined outside of the body itself. All imports must be included in the function body itself! Imported packages must be available in the base image.\n",
    "\n",
    "Why? Because each component will be packaged as a Docker image. The base image must therefore contain all dependencies. Any dependencies you install manually in the notebook are invisible to the Python function once it is inside the image. The function itself becomes the entrypoint of the image, which is why all auxiliary functions must be defined inside the function. That does cause some unfortunate duplication, but it also means you do not have to worry about the mechanism of packaging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306c5d06-b834-4ccd-87fd-5bce83aaa213",
   "metadata": {},
   "source": [
    "For this pipeline, we can define three components:\n",
    "\n",
    "- Download the Flower data set\n",
    "- Train the TensorFlow model\n",
    "- Evaluate the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd770221-869f-40d1-8ea5-988c8d752b5d",
   "metadata": {},
   "source": [
    "##### Import Kubeflow SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "309b23c3-b389-44b7-93de-abe4cf50575d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "import kfp\n",
    "from kfp import dsl, components\n",
    "from kfp.components import InputBinaryFile, OutputBinaryFile, func_to_container_op, InputPath, OutputPath\n",
    "import time\n",
    "from functools import partial\n",
    "from kfserving import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb412ed-799f-44dc-ad4b-eb2ea2faec14",
   "metadata": {},
   "source": [
    "Define a fucntion to converts a Python function to a component and returns a task using `kfp.components.func_to_container_op()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58e7d82d-f5bb-4ab0-bf1c-f39b635aef60",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_to_container_op = partial(\n",
    "    components.func_to_container_op,\n",
    "    base_image='zdou001/only_tests:flower-nightly',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd2c1ad-7506-4c6f-9d0f-8d2e25a9cbe2",
   "metadata": {},
   "source": [
    "##### Component 1: Create standalone python function - load_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70d0ce38-0dec-4ecb-b8c4-a14e2676219c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@func_to_container_op\n",
    "def load_task(\n",
    "    dataset_url: str,\n",
    "    data_dir: OutputPath(str)\n",
    "):\n",
    "    \"\"\"Download flower data\"\"\"\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    import urllib.request\n",
    "    import tarfile\n",
    "\n",
    "    Path(data_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    thetarfile = dataset_url\n",
    "    ftpstream = urllib.request.urlopen(thetarfile)\n",
    "    thetarfile = tarfile.open(fileobj=ftpstream, mode=\"r|gz\")\n",
    "    thetarfile.extractall(data_dir)\n",
    "    print(f'data saved to {data_dir}/flower_photos')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05447f8-9d6a-4916-afcd-ff755564f496",
   "metadata": {},
   "source": [
    "##### Component 2: Create standalone python function - train_task()\n",
    "For both the training and evaluation, divide the integer-valued pixel values by 255 to scale all values into the [0, 1] (floating-point) range. This function must be copied into both component functions (cf. normalize_image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3383c24-9d9f-40d5-950b-14c25b713261",
   "metadata": {},
   "outputs": [],
   "source": [
    "@func_to_container_op\n",
    "def train_task(\n",
    "    data_dir: InputPath(str),\n",
    "    batch_size: int,\n",
    "    epochs: int,\n",
    "    model_dir: OutputPath(str)):\n",
    "\n",
    "    from pathlib import Path\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import PIL\n",
    "    import PIL.Image\n",
    "    import tensorflow as tf\n",
    "    import tensorflow_datasets as tfds\n",
    "\n",
    "    \"\"\"Load flower data using a Keras Utility\"\"\"\n",
    "    img_height = 180\n",
    "    img_width = 180\n",
    "    \n",
    "    data_path = data_dir + '/flower_photos'\n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "      data_path,\n",
    "      validation_split=0.2,\n",
    "      subset=\"training\",\n",
    "      seed=123,\n",
    "      image_size=(img_height, img_width),\n",
    "      batch_size=batch_size)\n",
    "\n",
    "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "      data_path,\n",
    "      validation_split=0.1,\n",
    "      subset=\"validation\",\n",
    "      seed=123,\n",
    "      image_size=(img_height, img_width),\n",
    "      batch_size=batch_size)\n",
    "\n",
    "    val_ds = val_ds.skip(12)\n",
    "\n",
    "    \"\"\"Standardize the data\"\"\"\n",
    "    normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "    normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "    image_batch, labels_batch = next(iter(normalized_ds))\n",
    "    first_image = image_batch[0]\n",
    "    # Notice the pixel values are now in `[0,1]`.\n",
    "    print(np.min(first_image), np.max(first_image))\n",
    "    \n",
    "    \"\"\"Configure the dataset for performance\"\"\"\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "    train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    \"\"\"Define the model\"\"\"\n",
    "    num_classes = 5\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Rescaling(1./255),\n",
    "      tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "      tf.keras.layers.MaxPooling2D(),\n",
    "      tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "      tf.keras.layers.MaxPooling2D(),\n",
    "      tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "      tf.keras.layers.MaxPooling2D(),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(128, activation='relu'),\n",
    "      tf.keras.layers.Dense(num_classes)\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "      optimizer='adam',\n",
    "      loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "      metrics=['accuracy'])\n",
    "    model.fit(\n",
    "      train_ds,\n",
    "      validation_data=val_ds,\n",
    "      epochs=epochs\n",
    "    )\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    Path(model_dir).mkdir(parents=True, exist_ok=True)\n",
    "    model.save(model_dir)\n",
    "    print(f'Model exported to: {model_dir}')\n",
    "    print(os.listdir(model_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923623e3-a145-42a3-9794-89bdbd08e121",
   "metadata": {},
   "source": [
    "##### Component 3: Create standalone python function - evaluate_task()\n",
    "Evaluate the model with the following Python function. The metrics metadata (loss and accuracy) is available to the Kubeflow Pipelines UI. All metadata can automatically be visualized with output viewer(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b00602fe-3cd1-43c7-971d-5c792225c3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "@func_to_container_op\n",
    "def evaluate_task(\n",
    "    data_dir: InputPath(str),\n",
    "    model_dir: InputPath(str),\n",
    "    batch_size: int,\n",
    "    mlpipeline_metrics_path: OutputPath('Metrics')):\n",
    "    \"\"\"Loads a saved model from file and uses a pre-downloaded dataset for evaluation.\n",
    "    Model metrics are persisted to `/mlpipeline-metrics.json` for Kubeflow Pipelines\n",
    "    metadata.\"\"\"\n",
    "    import tensorflow as tf\n",
    "    import tensorflow_hub as hub\n",
    "    import json\n",
    "    import os\n",
    "    from collections import namedtuple\n",
    "\n",
    "    \"\"\"Load test flower dataset using a Keras Utility\"\"\"\n",
    "    data_path = data_dir + '/flower_photos'\n",
    "    \n",
    "    img_height = 180\n",
    "    img_width = 180\n",
    "    test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "      data_path,\n",
    "      validation_split=0.1,\n",
    "      subset=\"validation\",\n",
    "      seed=123,\n",
    "      image_size=(img_height, img_width),\n",
    "      batch_size=batch_size)\n",
    "    test_ds = test_ds.take(12)\n",
    "\n",
    "    \"\"\"Configure the dataset for performance\"\"\"\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "  \n",
    "    # model_loading = os.path.join(model_dir, model_version)\n",
    "    reloaded = tf.keras.models.load_model(model_dir)\n",
    "    # Check its architecture\n",
    "    reloaded.summary()\n",
    "    eval_results = reloaded.evaluate(test_ds)\n",
    "    print('Test loss & accuracy: %s' % (eval_results,))\n",
    "    \n",
    "    # save results to json\n",
    "    metrics = []\n",
    "    for metric, value in zip(reloaded.metrics_names, eval_results):\n",
    "        metrics.append({\n",
    "            'name': metric,\n",
    "            'numberValue': round(value, 4),\n",
    "            'format': \"PERCENTAGE\"\n",
    "        })\n",
    "        \n",
    "    metrics_dict = {'metrics': metrics}\n",
    "    print(metrics_dict)\n",
    "\n",
    "    with open(mlpipeline_metrics_path, 'w') as f:\n",
    "        json.dump(metrics_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56803b7c-cfb9-4037-b02a-4adf2c5824d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. Combine the Components into a Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e694308a-2233-4af1-aefe-0622c0fa0fbc",
   "metadata": {},
   "source": [
    "Note that up to this point you have not yet used the Kubeflow Pipelines SDK!\n",
    "\n",
    "With the four components (i.e. self-contained functions) defined, wire up the dependencies with Kubeflow Pipelines.\n",
    "\n",
    "The call components.func_to_container_op(f, base_image=img)(*args) has the following ingredients:\n",
    "\n",
    "- `f` is the Python function that defines a component\n",
    "- `img` is the base (Docker) image used to package the function\n",
    "- `*arg`s lists the arguments to f\n",
    "\n",
    "What the `*args` mean is best explained by going forward through the graph:\n",
    "\n",
    "- `downloadOp` is the first step and has no dependencies; it therefore has no `InputPath`. Its output (i.e., `OutputPath`) is stored in `data_dir`\n",
    "- `trainOp` needs the data downloaded from `downloadOp` and its signature lists `data_dir` (input) and `model_dir` (output). It depends on `downloadOp.output` (i.e., the previous step’s output) and stores its own outputs in `model_dir`, which can be used by another step. `downloadOp` is the parent of `trainOp`, as required.\n",
    "- `evaluateOp`'s function takes three arguments: `data_dir` (i.e., `downloadOp.output`), `model_dir` (i.e., `trainOp.output`), and `metrics_path`, which is where the function stores its evaluation metrics. That way, `evaluateOp` can only run after the successful completion of both `downloadOp` and `trainOp`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe15ea01-eab9-406a-9e95-d0ee1805bd34",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Build Kubeflow Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e71d9c5-23ec-420d-a31b-585a93652505",
   "metadata": {},
   "source": [
    "Our next step will be to create the various components that will make up the pipeline. Define the pipeline using the *@dsl.pipeline* decorator.\n",
    "\n",
    "The pipeline function is defined and includes a number of paramters that will be fed into our various components throughout execution. Kubeflow Pipelines are created decalaratively. This means that the code is not run until the pipeline is compiled. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625f8b15-5bca-4a4d-9fea-47973d0834c5",
   "metadata": {},
   "source": [
    "Define the pipeline and define parameters to be fed into pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9655cef9-2bdb-48a8-8c37-8b34f9facf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name='Flower Classifier using Tensorflow',\n",
    "    description='Continues training a pretrained flower classification model, then tests serving it.',\n",
    ")\n",
    "def flower_classifier_pipeline(\n",
    "    dataset_url='https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
    "    batch_size=32,\n",
    "    epochs=3,\n",
    "    namespace=utils.get_default_target_namespace(),        \n",
    "):\n",
    "    downloadOp = load_task(dataset_url)\n",
    "\n",
    "    trainOp = train_task(downloadOp.output, batch_size, epochs)\n",
    "    trainOp.after(downloadOp)\n",
    "    # trainOp.container.set_gpu_limit(1)\n",
    "\n",
    "    evaluateOp = evaluate_task(downloadOp.output, trainOp.output, batch_size)\n",
    "    # evaluateOp.after(trainOp)\n",
    "    # evaluateOp.container.set_gpu_limit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ec10e4-043f-4fd5-bb89-59640272e819",
   "metadata": {},
   "source": [
    "##### Run pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7e615a-18de-4445-837f-7a523fd838e1",
   "metadata": {},
   "source": [
    "Finally we feed our pipeline definition into the compiler and run it as an experiment. This will give us 2 links at the bottom that we can follow to the [Kubeflow Pipelines UI](https://www.kubeflow.org/docs/pipelines/overview/pipelines-overview/) where you can check logs, artifacts, inputs/outputs, and visually see the progress of your pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40002434-830a-45a1-abb5-d0d5f6c7d3cc",
   "metadata": {},
   "source": [
    "Create a client to enable communication with the Pipelines API server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2821fb25-2a95-4fc3-98b9-c8e05d18c6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[E 220920 13:00:01 _satvolumecredentials:51] Failed to read a token from file '/var/run/secrets/kubeflow/pipelines/token' ([Errno 2] No such file or directory: '/var/run/secrets/kubeflow/pipelines/token').\n",
      "[W 220920 13:00:01 _client:372] Failed to set up default credentials. Proceeding without credentials...\n"
     ]
    }
   ],
   "source": [
    "client = kfp.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5a481b-b0e9-47a3-b734-8d4e9b1fb193",
   "metadata": {},
   "source": [
    "Compile and Run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1ae0630-52ce-4b95-815f-b64b9785917c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 220920 13:02:53 _client:457] Creating experiment flower_classifier_pipeline_student-02.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/b994e8fc-9b7b-4dd1-9406-f109fa1364b5\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/01385260-a7a5-42f3-8211-69b18c3e78eb\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kfp.compiler.Compiler().compile(flower_classifier_pipeline, 'tf_flower_classifier_pipeline.yaml')\n",
    "pipeline_func=flower_classifier_pipeline\n",
    "namespace = utils.get_default_target_namespace()\n",
    "experiment_name = 'flower_classifier_pipeline'+ \"_\"+ namespace\n",
    "run_name = pipeline_func.__name__ + ' run' \n",
    "run_result = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                              experiment_name=experiment_name, \n",
    "                                              run_name=run_name + '-' + time.strftime(\"%Y%m%d-%H%M%S\"), \n",
    "                                              arguments={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aab601-e51f-466d-8ab3-96031f26878d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
