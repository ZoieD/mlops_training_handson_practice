apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: flower-classifier-using-tensorflow-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.13, pipelines.kubeflow.org/pipeline_compilation_time: '2022-09-20T13:02:53.180746',
    pipelines.kubeflow.org/pipeline_spec: '{"description": "Continues training a pretrained
      flower classification model, then tests serving it.", "inputs": [{"default":
      "https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz",
      "name": "dataset_url", "optional": true}, {"default": "32", "name": "batch_size",
      "optional": true}, {"default": "3", "name": "epochs", "optional": true}, {"default":
      "student-02", "name": "namespace", "optional": true}], "name": "Flower Classifier
      using Tensorflow"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.13}
spec:
  entrypoint: flower-classifier-using-tensorflow
  templates:
  - name: evaluate-task
    container:
      args: [--data-dir, /tmp/inputs/data_dir/data, --model-dir, /tmp/inputs/model_dir/data,
        --batch-size, '{{inputs.parameters.batch_size}}', --mlpipeline-metrics, /tmp/outputs/mlpipeline_metrics/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def evaluate_task(
            data_dir,
            model_dir,
            batch_size,
            mlpipeline_metrics_path):
            """Loads a saved model from file and uses a pre-downloaded dataset for evaluation.
            Model metrics are persisted to `/mlpipeline-metrics.json` for Kubeflow Pipelines
            metadata."""
            import tensorflow as tf
            import tensorflow_hub as hub
            import json
            import os
            from collections import namedtuple

            """Load test flower dataset using a Keras Utility"""
            data_path = data_dir + '/flower_photos'

            img_height = 180
            img_width = 180
            test_ds = tf.keras.utils.image_dataset_from_directory(
              data_path,
              validation_split=0.1,
              subset="validation",
              seed=123,
              image_size=(img_height, img_width),
              batch_size=batch_size)
            test_ds = test_ds.take(12)

            """Configure the dataset for performance"""
            AUTOTUNE = tf.data.AUTOTUNE
            test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)

            # model_loading = os.path.join(model_dir, model_version)
            reloaded = tf.keras.models.load_model(model_dir)
            # Check its architecture
            reloaded.summary()
            eval_results = reloaded.evaluate(test_ds)
            print('Test loss & accuracy: %s' % (eval_results,))

            # save results to json
            metrics = []
            for metric, value in zip(reloaded.metrics_names, eval_results):
                metrics.append({
                    'name': metric,
                    'numberValue': round(value, 4),
                    'format': "PERCENTAGE"
                })

            metrics_dict = {'metrics': metrics}
            metrics_dict

            with open(mlpipeline_metrics_path, 'w') as f:
                json.dump(metrics_dict, f)

        import argparse
        _parser = argparse.ArgumentParser(prog='Evaluate task', description='Loads a saved model from file and uses a pre-downloaded dataset for evaluation.')
        _parser.add_argument("--data-dir", dest="data_dir", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--model-dir", dest="model_dir", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--batch-size", dest="batch_size", type=int, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--mlpipeline-metrics", dest="mlpipeline_metrics_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = evaluate_task(**_parsed_args)
      image: zdou001/only_tests:flower-nightly
    inputs:
      parameters:
      - {name: batch_size}
      artifacts:
      - {name: load-task-data_dir, path: /tmp/inputs/data_dir/data}
      - {name: train-task-model_dir, path: /tmp/inputs/model_dir/data}
    outputs:
      artifacts:
      - {name: mlpipeline-metrics, path: /tmp/outputs/mlpipeline_metrics/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.13
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Loads
          a saved model from file and uses a pre-downloaded dataset for evaluation.",
          "implementation": {"container": {"args": ["--data-dir", {"inputPath": "data_dir"},
          "--model-dir", {"inputPath": "model_dir"}, "--batch-size", {"inputValue":
          "batch_size"}, "--mlpipeline-metrics", {"outputPath": "mlpipeline_metrics"}],
          "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\" >
          \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef evaluate_task(\n    data_dir,\n    model_dir,\n    batch_size,\n    mlpipeline_metrics_path):\n    \"\"\"Loads
          a saved model from file and uses a pre-downloaded dataset for evaluation.\n    Model
          metrics are persisted to `/mlpipeline-metrics.json` for Kubeflow Pipelines\n    metadata.\"\"\"\n    import
          tensorflow as tf\n    import tensorflow_hub as hub\n    import json\n    import
          os\n    from collections import namedtuple\n\n    \"\"\"Load test flower
          dataset using a Keras Utility\"\"\"\n    data_path = data_dir + ''/flower_photos''\n\n    img_height
          = 180\n    img_width = 180\n    test_ds = tf.keras.utils.image_dataset_from_directory(\n      data_path,\n      validation_split=0.1,\n      subset=\"validation\",\n      seed=123,\n      image_size=(img_height,
          img_width),\n      batch_size=batch_size)\n    test_ds = test_ds.take(12)\n\n    \"\"\"Configure
          the dataset for performance\"\"\"\n    AUTOTUNE = tf.data.AUTOTUNE\n    test_ds
          = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n\n    # model_loading
          = os.path.join(model_dir, model_version)\n    reloaded = tf.keras.models.load_model(model_dir)\n    #
          Check its architecture\n    reloaded.summary()\n    eval_results = reloaded.evaluate(test_ds)\n    print(''Test
          loss & accuracy: %s'' % (eval_results,))\n\n    # save results to json\n    metrics
          = []\n    for metric, value in zip(reloaded.metrics_names, eval_results):\n        metrics.append({\n            ''name'':
          metric,\n            ''numberValue'': round(value, 4),\n            ''format'':
          \"PERCENTAGE\"\n        })\n\n    metrics_dict = {''metrics'': metrics}\n    metrics_dict\n\n    with
          open(mlpipeline_metrics_path, ''w'') as f:\n        json.dump(metrics_dict,
          f)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Evaluate
          task'', description=''Loads a saved model from file and uses a pre-downloaded
          dataset for evaluation.'')\n_parser.add_argument(\"--data-dir\", dest=\"data_dir\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-dir\",
          dest=\"model_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--batch-size\",
          dest=\"batch_size\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mlpipeline-metrics\",
          dest=\"mlpipeline_metrics_path\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = evaluate_task(**_parsed_args)\n"], "image": "zdou001/only_tests:flower-nightly"}},
          "inputs": [{"name": "data_dir", "type": "String"}, {"name": "model_dir",
          "type": "String"}, {"name": "batch_size", "type": "Integer"}], "name": "Evaluate
          task", "outputs": [{"name": "mlpipeline_metrics", "type": "Metrics"}]}',
        pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"batch_size":
          "{{inputs.parameters.batch_size}}"}'}
  - name: flower-classifier-using-tensorflow
    inputs:
      parameters:
      - {name: batch_size}
      - {name: dataset_url}
      - {name: epochs}
    dag:
      tasks:
      - name: evaluate-task
        template: evaluate-task
        dependencies: [load-task, train-task]
        arguments:
          parameters:
          - {name: batch_size, value: '{{inputs.parameters.batch_size}}'}
          artifacts:
          - {name: load-task-data_dir, from: '{{tasks.load-task.outputs.artifacts.load-task-data_dir}}'}
          - {name: train-task-model_dir, from: '{{tasks.train-task.outputs.artifacts.train-task-model_dir}}'}
      - name: load-task
        template: load-task
        arguments:
          parameters:
          - {name: dataset_url, value: '{{inputs.parameters.dataset_url}}'}
      - name: train-task
        template: train-task
        dependencies: [load-task]
        arguments:
          parameters:
          - {name: batch_size, value: '{{inputs.parameters.batch_size}}'}
          - {name: epochs, value: '{{inputs.parameters.epochs}}'}
          artifacts:
          - {name: load-task-data_dir, from: '{{tasks.load-task.outputs.artifacts.load-task-data_dir}}'}
  - name: load-task
    container:
      args: [--dataset-url, '{{inputs.parameters.dataset_url}}', --data-dir, /tmp/outputs/data_dir/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def load_task(
            dataset_url,
            data_dir
        ):
            """Download flower data"""
            import os
            from pathlib import Path
            import urllib.request
            import tarfile

            Path(data_dir).mkdir(parents=True, exist_ok=True)

            thetarfile = dataset_url
            ftpstream = urllib.request.urlopen(thetarfile)
            thetarfile = tarfile.open(fileobj=ftpstream, mode="r|gz")
            thetarfile.extractall(data_dir)
            print(f'data saved to {data_dir}/flower_photos')

        import argparse
        _parser = argparse.ArgumentParser(prog='Load task', description='Download flower data')
        _parser.add_argument("--dataset-url", dest="dataset_url", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--data-dir", dest="data_dir", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = load_task(**_parsed_args)
      image: zdou001/only_tests:flower-nightly
    inputs:
      parameters:
      - {name: dataset_url}
    outputs:
      artifacts:
      - {name: load-task-data_dir, path: /tmp/outputs/data_dir/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.13
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Download
          flower data", "implementation": {"container": {"args": ["--dataset-url",
          {"inputValue": "dataset_url"}, "--data-dir", {"outputPath": "data_dir"}],
          "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\" >
          \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef load_task(\n    dataset_url,\n    data_dir\n):\n    \"\"\"Download
          flower data\"\"\"\n    import os\n    from pathlib import Path\n    import
          urllib.request\n    import tarfile\n\n    Path(data_dir).mkdir(parents=True,
          exist_ok=True)\n\n    thetarfile = dataset_url\n    ftpstream = urllib.request.urlopen(thetarfile)\n    thetarfile
          = tarfile.open(fileobj=ftpstream, mode=\"r|gz\")\n    thetarfile.extractall(data_dir)\n    print(f''data
          saved to {data_dir}/flower_photos'')\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Load
          task'', description=''Download flower data'')\n_parser.add_argument(\"--dataset-url\",
          dest=\"dataset_url\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--data-dir\",
          dest=\"data_dir\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = load_task(**_parsed_args)\n"], "image": "zdou001/only_tests:flower-nightly"}},
          "inputs": [{"name": "dataset_url", "type": "String"}], "name": "Load task",
          "outputs": [{"name": "data_dir", "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"dataset_url": "{{inputs.parameters.dataset_url}}"}'}
  - name: train-task
    container:
      args: [--data-dir, /tmp/inputs/data_dir/data, --batch-size, '{{inputs.parameters.batch_size}}',
        --epochs, '{{inputs.parameters.epochs}}', --model-dir, /tmp/outputs/model_dir/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def train_task(
            data_dir,
            batch_size,
            epochs,
            model_dir):

            from pathlib import Path
            import numpy as np
            import os
            import PIL
            import PIL.Image
            import tensorflow as tf
            import tensorflow_datasets as tfds

            """Load flower data using a Keras Utility"""
            img_height = 180
            img_width = 180

            data_path = data_dir + '/flower_photos'
            train_ds = tf.keras.utils.image_dataset_from_directory(
              data_path,
              validation_split=0.2,
              subset="training",
              seed=123,
              image_size=(img_height, img_width),
              batch_size=batch_size)

            val_ds = tf.keras.utils.image_dataset_from_directory(
              data_path,
              validation_split=0.1,
              subset="validation",
              seed=123,
              image_size=(img_height, img_width),
              batch_size=batch_size)

            val_ds = val_ds.skip(12)

            """Standardize the data"""
            normalization_layer = tf.keras.layers.Rescaling(1./255)
            normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))
            image_batch, labels_batch = next(iter(normalized_ds))
            first_image = image_batch[0]
            # Notice the pixel values are now in `[0,1]`.
            print(np.min(first_image), np.max(first_image))

            """Configure the dataset for performance"""
            AUTOTUNE = tf.data.AUTOTUNE

            train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)
            val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)

            """Define the model"""
            num_classes = 5

            model = tf.keras.Sequential([
              tf.keras.layers.Rescaling(1./255),
              tf.keras.layers.Conv2D(32, 3, activation='relu'),
              tf.keras.layers.MaxPooling2D(),
              tf.keras.layers.Conv2D(32, 3, activation='relu'),
              tf.keras.layers.MaxPooling2D(),
              tf.keras.layers.Conv2D(32, 3, activation='relu'),
              tf.keras.layers.MaxPooling2D(),
              tf.keras.layers.Flatten(),
              tf.keras.layers.Dense(128, activation='relu'),
              tf.keras.layers.Dense(num_classes)
            ])

            model.compile(
              optimizer='adam',
              loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
            model.fit(
              train_ds,
              validation_data=val_ds,
              epochs=epochs
            )

            model.summary()

            Path(model_dir).mkdir(parents=True, exist_ok=True)
            model.save(model_dir)
            print(f'Model exported to: {model_dir}')
            print(os.listdir(model_dir))

        import argparse
        _parser = argparse.ArgumentParser(prog='Train task', description='')
        _parser.add_argument("--data-dir", dest="data_dir", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--batch-size", dest="batch_size", type=int, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--epochs", dest="epochs", type=int, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--model-dir", dest="model_dir", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = train_task(**_parsed_args)
      image: zdou001/only_tests:flower-nightly
    inputs:
      parameters:
      - {name: batch_size}
      - {name: epochs}
      artifacts:
      - {name: load-task-data_dir, path: /tmp/inputs/data_dir/data}
    outputs:
      artifacts:
      - {name: train-task-model_dir, path: /tmp/outputs/model_dir/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.13
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--data-dir", {"inputPath": "data_dir"}, "--batch-size", {"inputValue":
          "batch_size"}, "--epochs", {"inputValue": "epochs"}, "--model-dir", {"outputPath":
          "model_dir"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef train_task(\n    data_dir,\n    batch_size,\n    epochs,\n    model_dir):\n\n    from
          pathlib import Path\n    import numpy as np\n    import os\n    import PIL\n    import
          PIL.Image\n    import tensorflow as tf\n    import tensorflow_datasets as
          tfds\n\n    \"\"\"Load flower data using a Keras Utility\"\"\"\n    img_height
          = 180\n    img_width = 180\n\n    data_path = data_dir + ''/flower_photos''\n    train_ds
          = tf.keras.utils.image_dataset_from_directory(\n      data_path,\n      validation_split=0.2,\n      subset=\"training\",\n      seed=123,\n      image_size=(img_height,
          img_width),\n      batch_size=batch_size)\n\n    val_ds = tf.keras.utils.image_dataset_from_directory(\n      data_path,\n      validation_split=0.1,\n      subset=\"validation\",\n      seed=123,\n      image_size=(img_height,
          img_width),\n      batch_size=batch_size)\n\n    val_ds = val_ds.skip(12)\n\n    \"\"\"Standardize
          the data\"\"\"\n    normalization_layer = tf.keras.layers.Rescaling(1./255)\n    normalized_ds
          = train_ds.map(lambda x, y: (normalization_layer(x), y))\n    image_batch,
          labels_batch = next(iter(normalized_ds))\n    first_image = image_batch[0]\n    #
          Notice the pixel values are now in `[0,1]`.\n    print(np.min(first_image),
          np.max(first_image))\n\n    \"\"\"Configure the dataset for performance\"\"\"\n    AUTOTUNE
          = tf.data.AUTOTUNE\n\n    train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n    val_ds
          = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n\n    \"\"\"Define the
          model\"\"\"\n    num_classes = 5\n\n    model = tf.keras.Sequential([\n      tf.keras.layers.Rescaling(1./255),\n      tf.keras.layers.Conv2D(32,
          3, activation=''relu''),\n      tf.keras.layers.MaxPooling2D(),\n      tf.keras.layers.Conv2D(32,
          3, activation=''relu''),\n      tf.keras.layers.MaxPooling2D(),\n      tf.keras.layers.Conv2D(32,
          3, activation=''relu''),\n      tf.keras.layers.MaxPooling2D(),\n      tf.keras.layers.Flatten(),\n      tf.keras.layers.Dense(128,
          activation=''relu''),\n      tf.keras.layers.Dense(num_classes)\n    ])\n\n    model.compile(\n      optimizer=''adam'',\n      loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n      metrics=[''accuracy''])\n    model.fit(\n      train_ds,\n      validation_data=val_ds,\n      epochs=epochs\n    )\n\n    model.summary()\n\n    Path(model_dir).mkdir(parents=True,
          exist_ok=True)\n    model.save(model_dir)\n    print(f''Model exported to:
          {model_dir}'')\n    print(os.listdir(model_dir))\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Train task'', description='''')\n_parser.add_argument(\"--data-dir\",
          dest=\"data_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--batch-size\",
          dest=\"batch_size\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--epochs\",
          dest=\"epochs\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-dir\",
          dest=\"model_dir\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = train_task(**_parsed_args)\n"], "image": "zdou001/only_tests:flower-nightly"}},
          "inputs": [{"name": "data_dir", "type": "String"}, {"name": "batch_size",
          "type": "Integer"}, {"name": "epochs", "type": "Integer"}], "name": "Train
          task", "outputs": [{"name": "model_dir", "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"batch_size": "{{inputs.parameters.batch_size}}",
          "epochs": "{{inputs.parameters.epochs}}"}'}
  arguments:
    parameters:
    - {name: dataset_url, value: 'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz'}
    - {name: batch_size, value: '32'}
    - {name: epochs, value: '3'}
    - {name: namespace, value: student-02}
  serviceAccountName: pipeline-runner
